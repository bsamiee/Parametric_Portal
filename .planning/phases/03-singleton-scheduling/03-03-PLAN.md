---
phase: 03-singleton-scheduling
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - packages/server/src/infra/cluster.ts
autonomous: true

must_haves:
  truths:
    - "Entity handlers wrap execution with withinCluster({ entityId, entityType, shardId })"
    - "checkSingletonHealth function validates heartbeat staleness"
    - "Singleton considered unhealthy if no execution in 2x expected interval"
  artifacts:
    - path: "packages/server/src/infra/cluster.ts"
      provides: "Entity withinCluster wrapping, checkSingletonHealth utility"
      contains: "checkSingletonHealth"
  key_links:
    - from: "packages/server/src/infra/cluster.ts"
      to: "packages/server/src/context.ts"
      via: "Context.Request.withinCluster in entity handler"
      pattern: "withinCluster.*entityId"
      verify_first: "grep -n 'withinCluster' packages/server/src/context.ts"
---

<objective>
Add entity handler withinCluster wrapping and singleton health check utilities.

Purpose: Complete cluster context propagation for entity handlers and enable dead man's switch health monitoring.
Output: Entity handlers wrapped with withinCluster, checkSingletonHealth utility for health integration (Phase 8).
</objective>

<execution_context>
@/Users/bardiasamiee/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bardiasamiee/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-singleton-scheduling/03-RESEARCH.md
@.planning/phases/03-singleton-scheduling/03-01-SUMMARY.md
@packages/server/src/infra/cluster.ts
@packages/server/src/context.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wrap entity handlers with withinCluster context</name>
  <files>packages/server/src/infra/cluster.ts</files>
  <action>
Update ClusterEntityLive to wrap handler execution with `Context.Request.withinCluster`. The entity handler already has access to `currentAddress` with entityId, shardId, etc.

**Wrapping scope clarification**: The `withinCluster` wrapper encompasses the ENTIRE handler including:
- The inner Effect.gen body (processing logic)
- The Effect.ensuring finalizer
- The Effect.matchCauseEffect error transformation

This ensures cluster context is available throughout the entire processing pipeline, including error handlers.

Modify the `process` handler in ClusterEntityLive:

```typescript
const ClusterEntityLive = ClusterEntity.toLayer(Effect.gen(function* () {
  const currentAddress = yield* Entity.CurrentAddress;
  const initTs = yield* Clock.currentTimeMillis;
  const stateRef = yield* Ref.make(EntityState.idle(initTs));
  return {
    // withinCluster wraps ENTIRE handler: gen body + ensuring + matchCauseEffect
    process: (envelope) => Context.Request.withinCluster({
      entityId: currentAddress.entityId,
      entityType: currentAddress.entityType,
      shardId: currentAddress.shardId,
    })(
      Effect.gen(function* () {
        const ts = yield* Clock.currentTimeMillis;
        yield* Ref.set(stateRef, EntityState.processing(ts));
        yield* Effect.logDebug('Entity processing', { entityId: currentAddress.entityId, idempotencyKey: envelope.payload.idempotencyKey });
        const completeTs = yield* Clock.currentTimeMillis;
        yield* Ref.set(stateRef, new EntityState({ status: 'complete', updatedAt: completeTs }));
      }).pipe(
        Effect.ensuring(Clock.currentTimeMillis.pipe(Effect.flatMap((ts) => Ref.update(stateRef, (s) => new EntityState({ ...s, updatedAt: ts }))))),
        Effect.matchCauseEffect({
          onFailure: (cause) => Effect.fail(new EntityProcessError({
            cause,
            message: B.match(Chunk.isNonEmpty(Cause.defects(cause)), { onFalse: () => Cause.pretty(cause), onTrue: () => 'Internal error' }),
          })),
          onSuccess: Effect.succeed,
        }),
      ),
    ),
    status: () => Ref.get(stateRef).pipe(Effect.map((s) => new StatusResponse({ status: s.status, updatedAt: s.updatedAt }))),
  };
}), {
  // ... existing options unchanged
});
```

**Key changes**:
1. `withinCluster` wraps the ENTIRE Effect chain (gen + ensuring + matchCauseEffect)
2. Cluster context available in error handlers for logging/telemetry
3. Finalizers execute within cluster context for proper cleanup tracing

Note: `currentAddress` is of type `Entity.CurrentAddress` which has:
- `entityId: string`
- `entityType: string`
- `shardId: ShardId`

**Verification step** (before implementing): Confirm Context.Request.withinCluster exists:
```bash
grep -n "withinCluster" packages/server/src/context.ts
```

This satisfies success criteria #10: "Entity handlers wrap execution with `Context.Request.withinCluster({ entityId, entityType, shardId })`"
  </action>
  <verify>
`grep -n "withinCluster.*entityId" packages/server/src/infra/cluster.ts` shows entity context wrapping.
`grep -n "currentAddress.entityType" packages/server/src/infra/cluster.ts` shows entityType used.
  </verify>
  <done>
Entity handlers wrap execution with withinCluster providing entityId, entityType, and shardId to downstream code.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add checkSingletonHealth utility function</name>
  <files>packages/server/src/infra/cluster.ts</files>
  <action>
Add health check utilities in a new [HEALTH] section before [SERVICE]:

```typescript
// --- [HEALTH] ----------------------------------------------------------------

// Staleness check — Number.between for self-documenting range validation
const _checkStaleness = (intervalMs: number, lastExecMs: number) =>
  Clock.currentTimeMillis.pipe(
    Effect.map((now) => {
      const elapsed = now - lastExecMs;
      const threshold = intervalMs * _CONFIG.singleton.threshold;
      return Number.between({ minimum: 0, maximum: threshold })(elapsed);
    }),
  );

// Singleton health — validates heartbeat against expected interval
const checkSingletonHealth = (config: ReadonlyArray<{ readonly name: string; readonly expectedIntervalMs: number }>) =>
  Effect.gen(function* () {
    const metrics = yield* MetricsService;
    const results = yield* Effect.forEach(config, ({ name, expectedIntervalMs }) =>
      Metric.value(Metric.taggedWithLabels(
        metrics.singleton.lastExecution,
        MetricsService.label({ singleton: name }),
      )).pipe(
        Effect.flatMap((state: { readonly value: number }) =>
          _checkStaleness(expectedIntervalMs, state.value).pipe(
            Effect.map((healthy) => ({
              name,
              healthy,
              lastExecution: B.match(state.value > 0, {
                onTrue: () => new Date(state.value).toISOString(),
                onFalse: () => 'never',
              }),
              staleMs: B.match(state.value > 0, {
                onTrue: () => Date.now() - state.value,
                onFalse: () => -1,
              }),
            })),
          ),
        ),
      ),
    );
    const [healthy, unhealthy] = A.partition(results, (r) => r.healthy);
    return {
      singletons: results,
      healthy: A.isEmptyArray(unhealthy),
      healthyCount: healthy.length,
      unhealthyCount: unhealthy.length,
    };
  });
```

Add `Number` import from effect (check if already imported with other effect utilities).

This utility:
1. Checks each singleton's heartbeat gauge against expected interval
2. Uses `_CONFIG.singleton.threshold` (default: 2x) for staleness calculation
3. Returns structured health result for Phase 8 integration
4. Uses `Array.partition` for single-pass healthy/unhealthy split
  </action>
  <verify>
`grep -n "checkSingletonHealth" packages/server/src/infra/cluster.ts` shows function exists.
`grep -n "_checkStaleness" packages/server/src/infra/cluster.ts` shows helper exists.
`grep -n "Number.between" packages/server/src/infra/cluster.ts` shows range validation.
  </verify>
  <done>
checkSingletonHealth utility validates heartbeat staleness. Returns structured health result with healthy/unhealthy counts.
  </done>
</task>

<task type="auto">
  <name>Task 3: Export health utilities via ClusterService</name>
  <files>packages/server/src/infra/cluster.ts</files>
  <action>
Add health check export to ClusterService class:

```typescript
// Add to ClusterService class statics:
static readonly checkSingletonHealth = checkSingletonHealth;
```

Add type for health result to namespace:

```typescript
namespace ClusterService {
  // ... existing types ...
  export interface SingletonHealthResult {
    readonly singletons: ReadonlyArray<{
      readonly name: string;
      readonly healthy: boolean;
      readonly lastExecution: string;
      readonly staleMs: number;
    }>;
    readonly healthy: boolean;
    readonly healthyCount: number;
    readonly unhealthyCount: number;
  }
}
```

This enables Phase 8 to call `ClusterService.checkSingletonHealth(config)` for readiness probe integration.
  </action>
  <verify>
`grep -n "checkSingletonHealth" packages/server/src/infra/cluster.ts` shows static export.
`grep -n "SingletonHealthResult" packages/server/src/infra/cluster.ts` shows type in namespace.
`pnpm exec nx run server:typecheck` passes.
  </verify>
  <done>
ClusterService.checkSingletonHealth exported. SingletonHealthResult type available for consumers.
  </done>
</task>

</tasks>

<verification>
1. `pnpm exec nx run server:typecheck` passes with no errors
2. `grep -c "withinCluster" packages/server/src/infra/cluster.ts` returns 3+ occurrences (entity + singleton + cron)
3. `grep -c "checkSingletonHealth" packages/server/src/infra/cluster.ts` returns 2+ occurrences
4. `grep -c "Number.between" packages/server/src/infra/cluster.ts` returns 1+ occurrences
</verification>

<success_criteria>
- Entity handlers wrap with withinCluster({ entityId, entityType, shardId }) (success criteria #10)
- checkSingletonHealth validates heartbeat staleness using _CONFIG.singleton.threshold
- Health result includes per-singleton status with lastExecution timestamp
- Unhealthy detection when no execution in 2x expected interval (success criteria #6)
- Typecheck passes
</success_criteria>

<output>
After completion, create `.planning/phases/03-singleton-scheduling/03-03-SUMMARY.md`
</output>
