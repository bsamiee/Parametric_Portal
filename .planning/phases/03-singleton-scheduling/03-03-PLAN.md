---
phase: 03-singleton-scheduling
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - packages/server/src/infra/cluster.ts
autonomous: true

must_haves:
  truths:
    - "Entity handlers wrap execution with withinCluster({ entityId, entityType, shardId })"
    - "checkSingletonHealth validates heartbeat staleness via Clock.currentTimeMillis"
    - "checkClusterHealth aggregates ClusterMetrics.* gauges for cluster-wide status"
    - "Singleton unhealthy if no execution in 2x expected interval"
    - "All time access uses Clock.currentTimeMillis (testable)"
    - "DateTime.distanceDuration returns Duration directly for staleness"
    - "Duration.format for human-readable health logs"
    - "Array.partition for single-pass healthy/unhealthy split"
    - "N.between for self-documenting range validation"
    - "Effect.forEach({ concurrency: 'unbounded' }) for parallel health checks"
  artifacts:
    - path: "packages/server/src/infra/cluster.ts"
      provides: "Entity withinCluster wrapping, checkSingletonHealth utility"
      contains: "checkSingletonHealth"
  key_links:
    - from: "packages/server/src/infra/cluster.ts"
      to: "packages/server/src/context.ts"
      via: "Context.Request.withinCluster in entity handler"
      pattern: "withinCluster.*entityId"
    - from: "packages/server/src/infra/cluster.ts"
      to: "@effect/cluster"
      via: "ClusterMetrics for cluster-wide health gauges"
      pattern: "ClusterMetrics\\."
    - from: "packages/server/src/infra/cluster.ts"
      to: "effect"
      via: "Number.between for range validation"
      pattern: "N\\.between"
    - from: "packages/server/src/infra/cluster.ts"
      to: "effect"
      via: "DateTime.formatIso for timestamp formatting"
      pattern: "DateTime\\.formatIso"
---

<objective>
Add entity handler withinCluster wrapping and singleton health check utilities.

Purpose: Complete cluster context propagation for entity handlers and enable dead man's switch health monitoring.
Output: Entity handlers wrapped with withinCluster, checkSingletonHealth utility for health integration (Phase 8).
</objective>

<execution_context>
@/Users/bardiasamiee/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bardiasamiee/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-singleton-scheduling/03-RESEARCH.md
@.planning/phases/03-singleton-scheduling/03-01-SUMMARY.md
@packages/server/src/infra/cluster.ts
@packages/server/src/context.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wrap entity handlers with withinCluster context</name>
  <files>packages/server/src/infra/cluster.ts</files>
  <action>
Update ClusterEntityLive to wrap handler execution with `Context.Request.withinCluster`.

**IMPORTS** (should exist from Plan 02):
```typescript
import { Context } from '../context.ts';
```

**Entity Handler Update** (modify ClusterEntityLive):
```typescript
const ClusterEntityLive = ClusterEntity.toLayer(Effect.gen(function* () {
  const currentAddress = yield* Entity.CurrentAddress;
  const initTs = yield* Clock.currentTimeMillis;
  const stateRef = yield* Ref.make(EntityState.idle(initTs));
  return {
    // withinCluster wraps ENTIRE handler: gen body + ensuring + matchCauseEffect
    process: (envelope) => Context.Request.withinCluster({
      entityId: currentAddress.entityId,
      entityType: currentAddress.entityType,
      shardId: currentAddress.shardId,
    })(
      Effect.gen(function* () {
        const ts = yield* Clock.currentTimeMillis;
        yield* Ref.set(stateRef, EntityState.processing(ts));
        yield* Effect.logDebug('Entity processing', { entityId: currentAddress.entityId, idempotencyKey: envelope.payload.idempotencyKey });
        const completeTs = yield* Clock.currentTimeMillis;
        yield* Ref.set(stateRef, new EntityState({ status: 'complete', updatedAt: completeTs }));
      }).pipe(
        Effect.ensuring(Clock.currentTimeMillis.pipe(Effect.flatMap((ts) => Ref.update(stateRef, (s) => new EntityState({ ...s, updatedAt: ts }))))),
        Effect.matchCauseEffect({
          onFailure: (cause) => Effect.fail(new EntityProcessError({
            cause,
            message: B.match(Chunk.isNonEmpty(Cause.defects(cause)), { onFalse: () => Cause.pretty(cause), onTrue: () => 'Internal error' }),
          })),
          onSuccess: Effect.succeed,
        }),
      ),
    ),
    status: () => Ref.get(stateRef).pipe(Effect.map((s) => new StatusResponse({ status: s.status, updatedAt: s.updatedAt }))),
  };
}), {
  concurrency: _CONFIG.entity.concurrency,
  defectRetryPolicy: Schedule.exponential(_CONFIG.retry.defect.base).pipe(Schedule.jittered, Schedule.intersect(Schedule.recurs(_CONFIG.retry.defect.maxAttempts)), Schedule.upTo(Duration.seconds(30))),
  disableFatalDefects: false,
  mailboxCapacity: _CONFIG.entity.mailboxCapacity,
  maxIdleTime: _CONFIG.entity.maxIdleTime,
  spanAttributes: { 'entity.service': 'cluster-infrastructure', 'entity.version': 'v1' },
});
```

**Wrapping Scope**: The `withinCluster` wrapper encompasses the ENTIRE handler:
- Inner Effect.gen body (processing logic)
- Effect.ensuring finalizer
- Effect.matchCauseEffect error transformation

This ensures cluster context is available throughout the entire processing pipeline, including error handlers and finalizers.

**currentAddress Properties**:
- `entityId: string` — Entity identifier
- `entityType: string` — Entity type name
- `shardId: ShardId` — Shard assignment
  </action>
  <verify>
`grep -n "withinCluster.*entityId" packages/server/src/infra/cluster.ts` shows entity context wrapping.
`grep -n "currentAddress.entityType" packages/server/src/infra/cluster.ts` shows entityType used.
  </verify>
  <done>
Entity handlers wrap execution with withinCluster providing entityId, entityType, and shardId to downstream code.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add checkSingletonHealth utility function</name>
  <files>packages/server/src/infra/cluster.ts</files>
  <action>
Add health check utilities in a new [HEALTH] section before [SERVICE].

**IMPORTS** (add to effect import from Plan 02 base):
```typescript
// Consolidated imports — DateTime.distanceDuration, Duration.format for health utilities
import { Array as A, Boolean as B, Cause, Chunk, Clock, Config, Data, DateTime, Duration, Effect, Exit, FiberMap, HashMap, HashSet, Layer, Match, Metric, Number as N, Option, Ref, Schedule, Schema as S, SynchronizedRef, Tuple } from 'effect';

// ClusterMetrics for cluster-wide health gauges:
import { ClusterMetrics } from '@effect/cluster';

// Telemetry (verify exists from existing cluster.ts):
import { Telemetry } from '../observe/telemetry.ts';
```

**Health Utilities** (place before [SERVICE] section):
```typescript
// --- [HEALTH] ----------------------------------------------------------------

// Staleness check — DateTime.distanceDuration for clean Duration arithmetic
// N.between for self-documenting range validation
const _checkStaleness = (intervalMs: number, lastExecMs: number) =>
  Clock.currentTimeMillis.pipe(
    Effect.map((now) => {
      const elapsed = DateTime.distanceDuration(
        DateTime.unsafeMake(now),
        DateTime.unsafeMake(lastExecMs),
      );
      const elapsedMs = Duration.toMillis(elapsed);
      const threshold = intervalMs * _CONFIG.singleton.threshold;
      return {
        healthy: N.between({ minimum: 0, maximum: threshold })(elapsedMs),
        elapsed,
        elapsedMs,
      };
    }),
  );

// Singleton health check — validates heartbeat against expected interval
const checkSingletonHealth = (config: ReadonlyArray<{ readonly name: string; readonly expectedIntervalMs: number }>) =>
  Telemetry.span(Effect.gen(function* () {
      const metrics = yield* MetricsService;

      // Effect.forEach with concurrency for parallel health checks
      const results = yield* Effect.forEach(config, ({ name, expectedIntervalMs }) =>
        Metric.value(Metric.taggedWithLabels(
          metrics.singleton.lastExecution,
          MetricsService.label({ singleton: name }),
        )).pipe(
          Effect.flatMap((state: { readonly value: number }) =>
            _checkStaleness(expectedIntervalMs, state.value).pipe(
              Effect.map((staleness) => ({
                name,
                healthy: staleness.healthy,
                lastExecution: B.match(state.value > 0, {
                  onTrue: () => DateTime.formatIso(DateTime.unsafeMake(state.value)),
                  onFalse: () => 'never',
                }),
                // Duration.format for human-readable staleness: "2h 30m"
                staleFormatted: B.match(state.value > 0, {
                  onTrue: () => Duration.format(staleness.elapsed),
                  onFalse: () => 'N/A',
                }),
                staleMs: staleness.elapsedMs,
              })),
            ),
          ),
        ),
      { concurrency: 'unbounded' });

      // Array.partition for single-pass healthy/unhealthy split
      const [healthy, unhealthy] = A.partition(results, (r) => r.healthy);

      return {
        singletons: results,
        healthy: A.isEmptyArray(unhealthy),
        healthyCount: healthy.length,
        unhealthyCount: unhealthy.length,
      };
    }), 'cluster.checkSingletonHealth');

// Cluster-wide health aggregation — uses ClusterMetrics official gauges
// Uses Telemetry.span for tracing (matches codebase pattern)
const checkClusterHealth = () =>
  Telemetry.span(Effect.all({
    entities: Metric.value(ClusterMetrics.entities),
    singletons: Metric.value(ClusterMetrics.singletons),
    runners: Metric.value(ClusterMetrics.runners),
    runnersHealthy: Metric.value(ClusterMetrics.runnersHealthy),
    shards: Metric.value(ClusterMetrics.shards),
  }).pipe(
    Effect.map((m) => ({
      // Convert bigint gauge values to numbers for JSON serialization
      healthy: Number(m.runnersHealthy.value) > 0 && Number(m.singletons.value) > 0,
      degraded: Number(m.runnersHealthy.value) < Number(m.runners.value),
      metrics: {
        entities: Number(m.entities.value),
        runners: Number(m.runners.value),
        runnersHealthy: Number(m.runnersHealthy.value),
        shards: Number(m.shards.value),
        singletons: Number(m.singletons.value),
      },
    })),
  ), 'cluster.checkClusterHealth');
```

**Key Patterns**:
1. `Clock.currentTimeMillis` — Effect-native time access (testable via Clock layer mocking)
2. `DateTime.distanceDuration` — Returns Duration directly for staleness calculation
3. `Duration.format` — Human-readable staleness: "2h 30m" for health UI/logs
4. `N.between` — Self-documenting range validation for staleness threshold
5. `Array.partition` — Single-pass split of healthy/unhealthy results
6. `Boolean.match` — Binary condition handling for lastExecution formatting
7. `Effect.forEach({ concurrency: 'unbounded' })` — Parallel health checks
8. `Telemetry.span` — Named tracing (codebase pattern)
9. `ClusterMetrics.*` — Official cluster gauges (entities, runners, shards)
10. `_CONFIG.singleton.threshold` — Multiplier for staleness (default: 2x interval)
  </action>
  <verify>
`grep -n "checkSingletonHealth" packages/server/src/infra/cluster.ts` shows function exists.
`grep -n "_checkStaleness" packages/server/src/infra/cluster.ts` shows helper exists.
`grep -n "DateTime.distanceDuration" packages/server/src/infra/cluster.ts` shows Duration-based staleness.
`grep -n "Duration.format" packages/server/src/infra/cluster.ts` shows human-readable formatting.
`grep -n "N.between" packages/server/src/infra/cluster.ts` shows range validation.
`grep -n "concurrency: 'unbounded'" packages/server/src/infra/cluster.ts` shows parallel health checks.
  </verify>
  <done>
checkSingletonHealth utility validates heartbeat staleness using Clock.currentTimeMillis. Returns structured health result with healthy/unhealthy counts.
  </done>
</task>

<task type="auto">
  <name>Task 3: Export health utilities via ClusterService</name>
  <files>packages/server/src/infra/cluster.ts</files>
  <action>
Add health check exports to ClusterService class:

```typescript
// Add to ClusterService class statics:
static readonly checkSingletonHealth = checkSingletonHealth;
static readonly checkClusterHealth = checkClusterHealth;
```

Add types for health results to namespace:

```typescript
namespace ClusterService {
  // ... existing types ...
  export interface SingletonHealthResult {
    readonly singletons: ReadonlyArray<{
      readonly name: string;
      readonly healthy: boolean;
      readonly lastExecution: string;
      readonly staleFormatted: string;  // Duration.format output: "2h 30m"
      readonly staleMs: number;
    }>;
    readonly healthy: boolean;
    readonly healthyCount: number;
    readonly unhealthyCount: number;
  }

  export interface ClusterHealthResult {
    readonly healthy: boolean;
    readonly degraded: boolean;
    readonly metrics: {
      readonly entities: number;
      readonly runners: number;
      readonly runnersHealthy: number;
      readonly shards: number;
      readonly singletons: number;
    };
  }
}
```

This enables Phase 8 to call:
- `ClusterService.checkSingletonHealth(config)` for singleton heartbeat monitoring
- `ClusterService.checkClusterHealth()` for cluster-wide status (runners, shards, etc.)
  </action>
  <verify>
`grep -n "checkSingletonHealth" packages/server/src/infra/cluster.ts` shows static export.
`grep -n "checkClusterHealth" packages/server/src/infra/cluster.ts` shows static export.
`grep -n "SingletonHealthResult" packages/server/src/infra/cluster.ts` shows type in namespace.
`grep -n "ClusterHealthResult" packages/server/src/infra/cluster.ts` shows type in namespace.
`pnpm exec nx run server:typecheck` passes.
  </verify>
  <done>
ClusterService.checkSingletonHealth and checkClusterHealth exported. Health result types available for consumers.
  </done>
</task>

</tasks>

<verification>
1. `pnpm exec nx run server:typecheck` passes with no errors
2. `grep -c "withinCluster" packages/server/src/infra/cluster.ts` returns 3+ occurrences (entity + singleton + cron)
3. `grep -c "checkSingletonHealth" packages/server/src/infra/cluster.ts` returns 2+ occurrences
4. `grep -c "checkClusterHealth" packages/server/src/infra/cluster.ts` returns 2+ occurrences
5. `grep -c "DateTime.distanceDuration" packages/server/src/infra/cluster.ts` returns 1+ occurrences
6. `grep -c "Duration.format" packages/server/src/infra/cluster.ts` returns 1+ occurrences
7. `grep -c "N.between" packages/server/src/infra/cluster.ts` returns 1+ occurrences
8. `grep -c "A.partition" packages/server/src/infra/cluster.ts` returns 1+ occurrences
9. `grep -c "concurrency: 'unbounded'" packages/server/src/infra/cluster.ts` returns 1+ occurrences
10. `grep -c "ClusterMetrics" packages/server/src/infra/cluster.ts` returns 5+ occurrences
</verification>

<success_criteria>
- Entity handlers wrap with withinCluster({ entityId, entityType, shardId })
- checkSingletonHealth validates heartbeat staleness using _CONFIG.singleton.threshold
- checkClusterHealth aggregates ClusterMetrics.* gauges for cluster-wide status
- Health result includes per-singleton status with lastExecution, staleFormatted, staleMs
- DateTime.distanceDuration returns Duration directly for staleness calculation
- Duration.format provides human-readable staleness: "2h 30m"
- Effect.forEach with `{ concurrency: 'unbounded' }` for parallel health checks
- All time access uses Clock.currentTimeMillis (no Date.now)
- Array.partition for single-pass healthy/unhealthy split
- Telemetry.span for named tracing (codebase pattern)
- Typecheck passes
</success_criteria>

<output>
After completion, create `.planning/phases/03-singleton-scheduling/03-03-SUMMARY.md`
</output>
