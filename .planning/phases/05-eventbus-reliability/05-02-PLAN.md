---
phase: 05-eventbus-reliability
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - packages/server/src/infra/cluster.ts
  - packages/server/src/events/events.ts
  - apps/api/src/main.ts
autonomous: true

must_haves:
  truths:
    - "Full runner layers provide Sharding.Sharding and CurrentAddress"
    - "EventBus service emits typed domain events via broadcaster"
    - "Events persist to outbox within transaction scope"
    - "Duplicate events are detected and skipped"
    - "Failed events dead-letter after max retries"
    - "Outbox worker polls pending events and broadcasts"
    - "Startup recovery reprocesses unprocessed messages"
    - "Reactivity.mutation invalidates subscription keys on emit"
  artifacts:
    - path: "packages/server/src/events/events.ts"
      provides: "EventBus service with emit, subscribe, broadcaster, startup recovery"
      min_lines: 280
      max_lines: 350
      contains: "class EventBus"
  key_links:
    - from: "packages/server/src/infra/cluster.ts"
      to: "@effect/cluster"
      via: "SocketRunner.layer (full, not layerClientOnly)"
      pattern: "SocketRunner\\.layer[^C]"
    - from: "packages/server/src/events/events.ts"
      to: "@effect/cluster"
      via: "Sharding.broadcaster import"
      pattern: "Sharding\\.broadcaster"
    - from: "packages/server/src/events/events.ts"
      to: "@effect/experimental"
      via: "VariantSchema.make import"
      pattern: "VariantSchema\\.make"
    - from: "packages/server/src/events/events.ts"
      to: "packages/database/src/repos.ts"
      via: "DatabaseService.eventOutbox for outbox polling"
      pattern: "db\\.eventOutbox\\.takePending"
    - from: "packages/server/src/events/events.ts"
      to: "@effect/cluster"
      via: "SqlMessageStorage for startup recovery"
      pattern: "SqlMessageStorage\\.unprocessedMessages"
    - from: "packages/server/src/events/events.ts"
      to: "@effect/experimental"
      via: "Reactivity.mutation for invalidation"
      pattern: "reactivity\\.mutation"
---

<objective>
EventBus service with typed domain events, transactional outbox, and cluster-wide broadcast.

Purpose: Replace StreamingService.channel() with typed, reliable event publishing. Events emit within database transactions, broadcast across pods via Sharding.broadcaster, deduplicate via PersistedCache, and dead-letter on exhausted retries.

Output:
- Single DomainEvent VariantSchema with dot-notation event types
- EventError with reason discriminant (DeliveryFailed, DuplicateEvent, etc.)
- Polymorphic emit() with reactivity.mutation for auto-invalidation
- subscribe() with type-safe variant filtering
- Outbox worker polling DatabaseService.eventOutbox.takePending() and broadcasting
- Startup recovery via SqlMessageStorage.unprocessedMessages
</objective>

<execution_context>
@/Users/bardiasamiee/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bardiasamiee/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-eventbus-reliability/05-RESEARCH.md
@.planning/phases/05-eventbus-reliability/05-CONTEXT.md
@.planning/phases/05-eventbus-reliability/05-01-SUMMARY.md
@packages/server/src/infra/cluster.ts
@packages/server/src/infra/jobs.ts
@packages/server/src/observe/metrics.ts
@packages/server/src/utils/resilience.ts
@packages/database/src/repos.ts
</context>

<clarifications>
**Event Naming:** Dot-notation (`user.created`) for database eventType field. VariantSchema uses category tags (`_tag: 'user'`) + action field (`payload.action: 'created'`). Combined via getter: `${_tag}.${payload.action}`.

**VariantSchema Pattern:** `_VS = VariantSchema.make(...)` is CORRECT. Factory provides `_VS.Class()` and `_VS.Field()` methods. "No loose const" means don't extract `type X = ...` separately from schema.

**Broadcaster vs Entity.make:** Research recommends `Sharding.broadcaster` for fire-and-forget fan-out. `Entity.make` is for request/response routing. EVNT-01 satisfied via broadcaster.
</clarifications>

<tasks>

<task type="auto">
  <name>Task 0: Migrate to full runner layers (prerequisite)</name>
  <files>packages/server/src/infra/cluster.ts, apps/api/src/main.ts</files>
  <action>
Fix runner layer to provide Sharding.Sharding and CurrentAddress for entity hosting.

**Issue:** Current code uses `SocketRunner.layerClientOnly` / `HttpRunner.layerClient` (client-only). API server hosts entities (ClusterEntity, JobEntity) which requires full runner layers.

**In cluster.ts:**

1. Add import: `import { NodeSocketServer } from '@effect/platform-node';`

2. Add socket server config to _CONFIG:
```typescript
socketServer: { port: Config.integer('CLUSTER_SOCKET_PORT').pipe(Config.withDefault(9000)) },
```

3. Replace _transports definition - change `layerClientOnly` to full `layer`:
```typescript
const _transports = {
  auto: SocketRunner.layer.pipe(
    Layer.provide(NodeSocketServer.layer({ port: _CONFIG.socketServer.port })),
    Layer.provide(NodeClusterSocket.layerClientProtocol),
    Layer.provideMerge(_transportBase),
    Layer.catchAll((e) => Effect.logWarning('Socket unavailable, using HTTP', { error: String(e) }).pipe(Effect.as(_httpLayer), Layer.unwrapEffect))),
  http: _httpLayer,
  socket: SocketRunner.layer.pipe(
    Layer.provide(NodeSocketServer.layer({ port: _CONFIG.socketServer.port })),
    Layer.provide(NodeClusterSocket.layerClientProtocol),
    Layer.provideMerge(_transportBase)),
  websocket: HttpRunner.layer.pipe(
    Layer.provide(HttpRunner.layerProtocolWebsocketDefault),
    Layer.provide(NodeSocket.layerWebSocketConstructor),
    Layer.provide(FetchHttpClient.layer),
    Layer.provideMerge(_transportBase)),
} as const;
```

4. Update _clusterLayer to use Layer.provideMerge:
```typescript
const _clusterLayer = ClusterEntityLive.pipe(Layer.provideMerge(_transportLayer));
```

**In main.ts:**

5. Add import: `import { ClusterService } from '@parametric-portal/server/infra/cluster';`

6. Add ClusterService.Layer to ServicesLayer:
```typescript
Layer.provideMerge(Layer.mergeAll(DatabaseService.Default, SearchRepo.Default, MetricsService.Default, Crypto.Service.Default, Context.Request.SystemLayer, StreamingService.Default, ClusterService.Layer)),
```

Pattern: Full runner exposes Sharding.Sharding + CurrentAddress; Layer.provideMerge exposes ShardingConfig.
  </action>
  <verify>
1. Run `pnpm exec nx run-many -t typecheck` - No type errors for api and server packages
2. Effect passed to NodeRuntime.runMain has `R = never` (all requirements satisfied)
  </verify>
  <done>Full runner layers provide Sharding, CurrentAddress, ShardingConfig for entity hosting</done>
</task>

<task type="auto">
  <name>Task 1: Create EventBus service file with complete implementation</name>
  <files>packages/server/src/events/events.ts</files>
  <action>
Create the EventBus service following the research template. Target ~280-350 LOC.

**Directory:** Create `packages/server/src/events/` directory if it doesn't exist.

**Complete file implementation:**

```typescript
/**
 * EventBus: Typed domain events with transactional outbox and cluster broadcast.
 *
 * Architecture:
 * - DomainEvent via VariantSchema (_VS.Class + _VS.Field) - single source of truth
 * - Transactional outbox via DatabaseService.eventOutbox.offer() (called within db.transaction)
 * - Outbox worker polls db.eventOutbox.takePending() and broadcasts via Sharding.broadcaster
 * - Two-tier deduplication: in-memory LRU + persistent fallback via PersistedCache
 * - At-least-once delivery with dead-letter after max retries
 *
 * Event Naming:
 * - Database eventType: 'user.created', 'order.placed' (dot-notation)
 * - Schema _tag: 'user', 'order' (category discriminator)
 * - Schema payload.action: 'created', 'placed' (action within category)
 *
 * Routing: Uses Sharding.broadcaster for fire-and-forget fan-out (NOT Entity.make).
 * Research recommends broadcaster over Entity.make for event distribution.
 */
import { DeliverAt, RecipientType, Sharding, Snowflake, SqlMessageStorage } from '@effect/cluster';
import { PersistedCache, PersistedQueue, Reactivity, VariantSchema } from '@effect/experimental';
import { Activity, DurableClock, DurableRateLimiter } from '@effect/workflow';
import { Chunk, Clock, DateTime, Duration, Effect, Exit, HashSet, Match, Metric, Option, PrimaryKey, PubSub, Schedule, Schema as S, Stream } from 'effect';
import { DatabaseService } from '@parametric-portal/database/repos';
import { Context } from '../context.ts';
import { MetricsService } from '../observe/metrics.ts';
import { Telemetry } from '../observe/telemetry.ts';
import { Resilience } from '../utils/resilience.ts';
import { ClusterService } from '../infra/cluster.ts';

// --- [SCHEMA] ----------------------------------------------------------------

// VariantSchema factory: single source for variants, Class, Field
// NOTE: This is NOT "loose const extraction" - _VS is a factory providing Class/Field methods
const _VS = VariantSchema.make({ variants: ['order', 'payment', 'system', 'user'] as const, defaultVariant: 'system' });

// Error reason discriminant with Match.type for compile-time exhaustiveness
const EventErrorReason = S.Literal('DeliveryFailed', 'DeserializationFailed', 'DuplicateEvent', 'HandlerMissing', 'HandlerTimeout', 'MaxRetries', 'TransactionRollback', 'ValidationFailed');
const _errorProps = Match.type<typeof EventErrorReason.Type>().pipe(
  Match.when('DeliveryFailed', () => ({ retryable: true, terminal: false })),
  Match.when('DeserializationFailed', () => ({ retryable: false, terminal: true })),
  Match.when('DuplicateEvent', () => ({ retryable: false, terminal: true })),
  Match.when('HandlerMissing', () => ({ retryable: false, terminal: true })),
  Match.when('HandlerTimeout', () => ({ retryable: true, terminal: false })),
  Match.when('MaxRetries', () => ({ retryable: false, terminal: true })),
  Match.when('TransactionRollback', () => ({ retryable: false, terminal: true })),
  Match.when('ValidationFailed', () => ({ retryable: false, terminal: true })),
  Match.exhaustive,
);

class EventError extends S.TaggedError<EventError>()('EventError', {
  cause: S.optional(S.Unknown),
  eventId: S.optional(S.String),
  reason: EventErrorReason,
}) {
  get isTerminal(): boolean { return _errorProps(this.reason).terminal; }
  get isRetryable(): boolean { return _errorProps(this.reason).retryable; }
  static readonly from = (eventId: string, reason: typeof EventErrorReason.Type, cause?: unknown) => new EventError({ cause, eventId, reason });
}

// DomainEvent class via _VS.Class - creates Schema.Class with static variant accessors
class DomainEvent extends _VS.Class('DomainEvent')({
  eventId: S.UUID.pipe(S.brand('EventId')),
  aggregateId: S.String,
  correlationId: S.optional(S.UUID),
  causationId: S.optional(S.UUID),
  // NOTE: No occurredAt field - eventId (Snowflake) contains timestamp via Snowflake.timestamp()
  payload: _VS.Field({
    order: S.Struct({ action: S.Literal('placed', 'shipped', 'delivered', 'cancelled'), orderId: S.UUID, status: S.String, items: S.Array(S.Struct({ sku: S.String, qty: S.Number })) }),
    payment: S.Struct({ action: S.Literal('initiated', 'completed', 'failed', 'refunded'), paymentId: S.UUID, amount: S.Number, currency: S.String }),
    system: S.Struct({ action: S.Literal('started', 'stopped', 'health'), details: S.optional(S.Unknown) }),
    user: S.Struct({ action: S.Literal('created', 'updated', 'deleted'), userId: S.UUID, email: S.optional(S.String), changes: S.optional(S.Unknown) }),
  }),
}) {
  [PrimaryKey.symbol]() { return `event:${this.eventId}`; }
  /** Dot-notation event type for database storage: 'user.created', 'order.placed' */
  get eventType(): string { return `${this.payload._tag}.${this.payload.action}`; }
}

// EventEnvelope wraps event with emit timestamp + trace context
class EventEnvelope extends S.Class<EventEnvelope>('EventEnvelope')({
  event: DomainEvent,
  emittedAt: S.DateTimeUtcFromNumber,
  traceContext: S.optional(S.Struct({ traceId: S.String, spanId: S.String, parentSpanId: S.optional(S.String) })),
}) {
  [PrimaryKey.symbol]() { return `envelope:${this.event.eventId}`; }
}

// --- [CONSTANTS] -------------------------------------------------------------

const _CONFIG = {
  batch: { maxSize: 100, window: Duration.millis(50) },
  broadcast: { bulkhead: 5, threshold: 3, timeout: Duration.millis(100) },
  buffer: { capacity: 256, replay: 16 },
  dedupe: { ttl: Duration.minutes(5), inMemoryCapacity: 10000, inMemoryTTL: Duration.minutes(5) },
  outbox: { pollInterval: Duration.seconds(1), batchSize: 100 },
  rateLimit: { window: Duration.seconds(1), limit: 100, algorithm: 'token-bucket' as const },
  retry: { maxAttempts: 5, timeout: Duration.seconds(30), backoffBase: Duration.millis(100) },
} as const;

// Default handler via Match.type - exhaustive, no dispatch table
const _handleDefault = Match.type<DomainEvent>().pipe(
  Match.tag('order', (e) => Match.value(e.payload.action).pipe(
    Match.when('placed', () => Effect.logInfo('Order placed', { orderId: e.payload.orderId })),
    Match.when('shipped', () => Effect.logInfo('Order shipped', { orderId: e.payload.orderId })),
    Match.when('delivered', () => Effect.logInfo('Order delivered', { orderId: e.payload.orderId })),
    Match.when('cancelled', () => Effect.logInfo('Order cancelled', { orderId: e.payload.orderId })),
    Match.exhaustive,
  )),
  Match.tag('user', (e) => Match.value(e.payload.action).pipe(
    Match.when('created', () => Effect.logInfo('User created', { userId: e.payload.userId })),
    Match.when('updated', () => Effect.logInfo('User updated', { userId: e.payload.userId })),
    Match.when('deleted', () => Effect.logInfo('User deleted', { userId: e.payload.userId })),
    Match.exhaustive,
  )),
  Match.tag('payment', () => Effect.void),
  Match.tag('system', () => Effect.void),
  Match.exhaustive,
);

// --- [SERVICE] ---------------------------------------------------------------

class EventBus extends Effect.Service<EventBus>()('server/EventBus', {
  dependencies: [ClusterService.Layer, DatabaseService.Default, MetricsService.Default, Reactivity.layer, PersistedCache.layer],
  scoped: Effect.gen(function* () {
    const sharding = yield* Sharding.Sharding;
    const db = yield* DatabaseService;
    const metrics = yield* MetricsService;
    const reactivity = yield* Reactivity.Reactivity;
    const statusHub = yield* PubSub.sliding<EventEnvelope>({ capacity: _CONFIG.buffer.capacity, replay: _CONFIG.buffer.replay });
    const broadcaster = yield* sharding.broadcaster(RecipientType.Topic('domain-events', EventEnvelope));

    // Two-tier dedupe: in-memory LRU (hot path O(1)) + persistent fallback (cold path)
    const dedupCache = yield* PersistedCache.make({
      storeId: 'event-processed',
      lookup: (key: EventBus.DedupKey) => Effect.succeed(undefined),
      timeToLive: (_key, exit) => Match.value(Exit.isSuccess(exit)).pipe(Match.when(true, () => Duration.hours(24)), Match.orElse(() => _CONFIG.dedupe.ttl)),
      inMemoryCapacity: _CONFIG.dedupe.inMemoryCapacity,
      inMemoryTTL: _CONFIG.dedupe.inMemoryTTL,
    });

    yield* Effect.annotateLogsScoped({ 'service.name': 'eventbus' });

    // Process envelope with Activity.make for replay-safe idempotency
    const _processEnvelope = (envelope: EventEnvelope, handler: (event: DomainEvent) => Effect.Effect<void, EventError>) =>
      Telemetry.span(
        Context.Request.withinCluster({ entityType: 'EventBus' })(
          dedupCache.get(new EventBus.DedupKey({ eventId: envelope.event.eventId })).pipe(
            Effect.flatMap(Option.match({
              onSome: () => Metric.increment(metrics.events.duplicatesSkipped).pipe(Effect.asVoid),
              onNone: () => Activity.make({
                name: `handler.${envelope.event.payload._tag}`,
                idempotencyKey: () => `${envelope.event.payload._tag}:${envelope.event.eventId}`,
                execute: Activity.CurrentAttempt.pipe(Effect.flatMap((attempt) =>
                  handler(envelope.event).pipe(Effect.timeout(Duration.millis(Duration.toMillis(_CONFIG.retry.timeout) * (attempt.attemptNumber + 1)))),
                )),
              }).pipe(
                Activity.retry({ times: _CONFIG.retry.maxAttempts }),
                Effect.tap(() => dedupCache.set(new EventBus.DedupKey({ eventId: envelope.event.eventId }), Exit.succeed(undefined))),
              ),
            })),
            Effect.catchAll((e) => Clock.currentTimeMillis.pipe(
              Effect.flatMap((ts) => db.deadLetter.insert({
                appId: 'system', attempts: 1, errorHistory: [{ error: String(e), timestamp: ts }],
                errorReason: 'MaxRetries', source: 'event', sourceId: envelope.event.eventId, type: envelope.event.eventType, payload: envelope,
              })),
              Effect.zipRight(Metric.increment(metrics.events.deadLettered)),
              Effect.asVoid,
            )),
          ),
        ),
        'eventbus.processEnvelope',
        { 'event.id': envelope.event.eventId, 'event.type': envelope.event.eventType },
      );

    // Circuit-protected broadcast
    const _broadcastCircuit = (envelope: EventEnvelope) => {
      const circuitName = `eventbus.broadcast.${envelope.event.payload._tag}`;
      return Resilience.run(circuitName, broadcaster.send(envelope), {
        bulkhead: _CONFIG.broadcast.bulkhead,
        circuit: circuitName,
        retry: false,
        threshold: _CONFIG.broadcast.threshold,
        timeout: _CONFIG.broadcast.timeout,
      });
    };

    // Outbox worker: polls DatabaseService.eventOutbox.takePending() and broadcasts
    // This is the bridge between transactional outbox (Plan 05-01) and cluster broadcast
    yield* Effect.forkScoped(
      Effect.gen(function* () {
        yield* Effect.logInfo('Outbox worker started');
        yield* Stream.repeatEffect(
          db.eventOutbox.takePending(_CONFIG.outbox.batchSize).pipe(
            Effect.flatMap((pending) => Chunk.fromIterable(pending).pipe(
              Chunk.match({
                onEmpty: () => Effect.void,
                onNonEmpty: (items) => Effect.forEach(items, (row) =>
                  S.decode(EventEnvelope)(row.payload).pipe(
                    Effect.flatMap((envelope) =>
                      DurableRateLimiter.rateLimit({ name: 'eventbus.broadcast', algorithm: _CONFIG.rateLimit.algorithm, window: _CONFIG.rateLimit.window, limit: _CONFIG.rateLimit.limit, key: `event:${envelope.event.payload._tag}` }).pipe(
                        Effect.zipRight(_broadcastCircuit(envelope)),
                        Effect.tap(() => db.eventOutbox.markPublished(row.id)),
                        Effect.tap(() => dedupCache.set(new EventBus.DedupKey({ eventId: envelope.event.eventId }), Exit.succeed(undefined))),
                        Effect.tap(() => Metric.increment(metrics.events.processed)),
                        Effect.catchAll((e) =>
                          Effect.logWarning('Outbox broadcast failed', { eventId: envelope.event.eventId, error: String(e) }).pipe(
                            Effect.zipRight(db.eventOutbox.markFailed(row.id)),
                            Effect.zipRight(Clock.currentTimeMillis.pipe(
                              Effect.flatMap((ts) => db.deadLetter.insert({
                                appId: 'system', attempts: 1, errorHistory: [{ error: String(e), timestamp: ts }],
                                errorReason: 'DeliveryFailed', source: 'event', sourceId: envelope.event.eventId, type: envelope.event.eventType, payload: envelope,
                              })),
                            )),
                          ),
                        ),
                      ),
                    ),
                    Effect.catchAll((e) => Effect.logError('Failed to decode outbox event', { rowId: row.id, error: String(e) })),
                  ),
                ),
              }),
            )),
          ),
        ).pipe(
          Stream.schedule(Schedule.spaced(_CONFIG.outbox.pollInterval)),
          Stream.runDrain,
        );
      }),
    );

    // Polymorphic emit: handles single event or Chunk via Match.value
    const emit = (input: DomainEvent | Chunk.Chunk<DomainEvent>, opts?: { scheduledAt?: number }) => {
      const items = Match.value(input).pipe(Match.when(Chunk.isChunk, (c) => c), Match.orElse((e) => Chunk.of(e)));
      return Telemetry.span(
        Effect.all([Context.Request.current, sharding.getSnowflake]).pipe(
          Effect.flatMap(([ctx, sf]) => {
            const deliverAt = Option.fromNullable(opts?.scheduledAt).pipe(Option.map(DateTime.unsafeMake));
            const enriched = Chunk.map(items, (e): EventEnvelope => ({
              event: { ...e, eventId: e.eventId ?? String(sf), correlationId: e.correlationId ?? ctx.requestId },
              emittedAt: DateTime.unsafeMake(Snowflake.timestamp(sf)),
              traceContext: Option.some({ traceId: ctx.traceId, spanId: ctx.spanId }),
              ...Option.match(deliverAt, { onNone: () => ({}), onSome: (dt) => ({ [DeliverAt.symbol]: () => dt }) }),
            }));
            // Compute invalidation keys for Reactivity
            const keys = HashSet.toArray(HashSet.fromIterable(Chunk.flatMap(enriched, (e) => [`events:${e.event.payload._tag}`, `aggregate:${e.event.aggregateId}`, 'events:all'])));
            // Offer to outbox within reactivity.mutation for auto-invalidation
            // MUST be called within db.transaction() by caller
            return reactivity.mutation(keys, Effect.forEach(enriched, (envelope) =>
              db.eventOutbox.offer({ appId: ctx.appId ?? 'system', eventId: envelope.event.eventId, eventType: envelope.event.eventType, payload: envelope }),
            )).pipe(
              Effect.zipRight(Effect.all([Metric.incrementBy(metrics.events.emitted, Chunk.size(enriched)), PubSub.publishAll(statusHub, Chunk.toArray(enriched))], { discard: true })),
            );
          }),
        ),
        'eventbus.emit',
        { 'event.batch': Chunk.size(items) > 1, 'event.count': Chunk.size(items) },
      );
    };

    // Subscribe returns Stream that re-executes on invalidation
    const subscribe = <T extends EventBus.Variant>(
      eventType: T,
      handler: EventBus.Handler<T>,
      options?: { filter?: (e: S.Schema.Type<(typeof DomainEvent)[T]>) => boolean },
    ) =>
      broadcaster.subscribe.pipe(
        Effect.map((queue) =>
          reactivity.stream([`events:${eventType}`, 'events:all'],
            Stream.fromQueue(queue).pipe(
              Stream.filter((e): e is EventEnvelope => e.event.payload._tag === eventType && Option.getOrElse(Option.fromNullable(options?.filter), () => () => true)(_VS.extract(eventType)(e.event))),
              Stream.debounce(Duration.millis(10)),
              Stream.bufferChunks({ capacity: _CONFIG.buffer.capacity, strategy: 'sliding' }),
              Stream.mapChunks((chunk) => Chunk.compact(Chunk.dedupe(chunk))),
              Stream.throttle({ units: _CONFIG.rateLimit.limit, duration: _CONFIG.rateLimit.window, strategy: 'enforce' }),
              Stream.mapEffect((chunk) => Chunk.forEach(chunk, (e) => _processEnvelope(e, handler as EventBus.Handler))),
            ),
          ),
        ),
        Stream.unwrap,
      );

    // Startup recovery: reprocess unprocessed messages from previous shutdown
    yield* Effect.all([sharding.getAssignedShardIds, Clock.currentTimeMillis]).pipe(
      Effect.flatMap(([shards, now]) => SqlMessageStorage.unprocessedMessages(shards, now)),
      Effect.flatMap((pending) => Chunk.fromIterable(pending).pipe(
        Chunk.match({
          onEmpty: () => Effect.void,
          onNonEmpty: (items) => broadcaster.sendAll(Chunk.toArray(items)).pipe(
            Effect.tap(() => Effect.logInfo('Recovered pending events', { count: Chunk.size(items) })),
          ),
        }),
      )),
      Effect.catchAll((e) => Effect.logWarning('Startup recovery failed', { error: String(e) })),
    );

    // Register default handler
    yield* Effect.forkScoped(
      broadcaster.subscribe.pipe(
        Effect.flatMap((queue) => Stream.fromQueue(queue).pipe(Stream.mapEffect((e) => _handleDefault(e.event)), Stream.runDrain)),
      ),
    ).pipe(Effect.tap(() => Effect.logInfo('Default event handler registered')));

    return { emit, subscribe, onEvent: () => Stream.fromPubSub(statusHub, { scoped: true }) };
  }),
}) {
  static readonly Config = _CONFIG;
  static readonly Error = EventError;
  static readonly Event = DomainEvent;
  static readonly Envelope = EventEnvelope;
  static readonly Topic = RecipientType.Topic('domain-events', EventEnvelope);
}
namespace EventBus {
  export type Variant = (typeof _VS.variants)[number];
  export type Event = DomainEvent;
  export type Envelope = EventEnvelope;
  export type Handler<T extends Variant = Variant> = (event: S.Schema.Type<(typeof DomainEvent)[T]>) => Effect.Effect<void, EventError>;
  export class DedupKey extends S.Class<DedupKey>('DedupKey')({ eventId: S.String }) {
    [PrimaryKey.symbol]() { return `dedup:${this.eventId}`; }
  }
}

// --- [EXPORT] ----------------------------------------------------------------

export { EventBus };
```

**Key implementation details:**

1. **Outbox worker**: Polls `db.eventOutbox.takePending()` on schedule, decodes payload, broadcasts via circuit-protected `_broadcastCircuit()`, marks published/failed, dead-letters on error.

2. **emit()**: Uses `reactivity.mutation()` with HashSet invalidation keys (`events:{tag}`, `aggregate:{id}`, `events:all`). Offers to `db.eventOutbox.offer()` - MUST be called within `db.transaction()` by caller.

3. **Startup recovery**: `SqlMessageStorage.unprocessedMessages()` reprocesses events pending from previous shutdown. CRITICAL for "durable by default" subscriptions per CONTEXT.

4. **eventType getter** on DomainEvent: Computes dot-notation from `_tag` + `payload.action`.
  </action>
  <verify>
1. Run `pnpm exec nx run server:typecheck` - No type errors
2. File is ~280-350 LOC (acceptable density for complete implementation)
3. EventBus exports: emit, subscribe, onEvent methods
4. VariantSchema used with _VS.Class, _VS.Field patterns
5. emit() uses reactivity.mutation() with HashSet invalidation keys
6. Startup recovery calls SqlMessageStorage.unprocessedMessages
7. Outbox worker calls db.eventOutbox.takePending() and broadcasts
  </verify>
  <done>EventBus service implements transactional outbox with cluster broadcast, outbox worker polling, and typed events</done>
</task>

<task type="auto">
  <name>Task 2: Verify metrics integration</name>
  <files>packages/server/src/observe/metrics.ts</files>
  <action>
Verify that metrics.ts already has the events.* metrics needed by EventBus.

Check for existence of:
- events.deadLettered (counter)
- events.deliveryLatency (histogram)
- events.duplicatesSkipped (counter)
- events.emitted (counter)
- events.outboxDepth (gauge)
- events.processed (counter)
- events.retries (counter)
- events.subscriptions (gauge)

If any are missing, add them following the existing pattern.

The research shows metrics.ts already has these defined - just verify they exist at correct paths.
  </action>
  <verify>
1. Check MetricsService.events namespace has all required metrics
2. Run `pnpm exec nx run server:typecheck`
  </verify>
  <done>Event metrics available for EventBus tracking</done>
</task>

</tasks>

<verification>
0. `pnpm exec nx run-many -t typecheck` - No type errors (runner layer fix)
1. `pnpm exec nx run server:typecheck` - No type errors
2. EventBus service exports emit, subscribe, onEvent
3. DomainEvent uses VariantSchema with order/payment/system/user variants
4. DomainEvent.eventType getter produces dot-notation ('user.created', 'order.placed')
5. EventError has reason discriminant with isTerminal/isRetryable getters
6. Outbox worker polls db.eventOutbox.takePending() and broadcasts
7. emit() uses reactivity.mutation() with HashSet invalidation keys
8. Startup recovery via SqlMessageStorage.unprocessedMessages
9. Broadcaster used for cross-pod fan-out (not Entity.make)
10. Metrics integration uses events.* counters/gauges
</verification>

<success_criteria>
- EventBus.emit() accepts single DomainEvent or Chunk (polymorphic)
- EventBus.emit() uses reactivity.mutation() for auto-invalidation
- EventBus.subscribe() returns typed Stream filtered by variant
- Outbox worker polls takePending() and broadcasts via Sharding.broadcaster
- Startup recovery reprocesses unprocessed messages from previous shutdown
- Duplicate events detected via PersistedCache and skipped
- Failed events dead-letter after configured max retries
- File ~280-320 LOC with const + namespace merge
</success_criteria>

<output>
After completion, create `.planning/phases/05-eventbus-reliability/05-02-SUMMARY.md`
</output>
