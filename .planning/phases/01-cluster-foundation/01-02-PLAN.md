---
phase: 01-cluster-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - packages/server/src/infra/cluster.ts
autonomous: true

must_haves:
  truths:
    - "Entity message sent on Pod A reaches handler on Pod B within 100ms"
    - "Shard ownership persists across pod restarts without message loss"
    - "Work claims via shard ownership without SELECT FOR UPDATE patterns"
    - "preemptiveShutdown: true configured for K8s graceful shutdown"
    - "Dedicated DB connection for RunnerStorage"
  artifacts:
    - path: "packages/server/src/infra/cluster.ts"
      provides: "Full ClusterService with Entity layer, storage layers, and sharding config"
      exports: ["ClusterService"]
      max_lines: 225
  key_links:
    - from: "packages/server/src/infra/cluster.ts"
      to: "@effect/cluster SqlRunnerStorage"
      via: "Layer.provide with dedicated PgClient"
      pattern: "SqlRunnerStorage\\.layer"
    - from: "packages/server/src/infra/cluster.ts"
      to: "@effect/cluster NodeClusterSocket"
      via: "Layer composition"
      pattern: "NodeClusterSocket\\.layer"
---

<objective>
Implement Entity layer, SQL storage backends, and cluster configuration for multi-pod coordination.

Purpose: Complete the ClusterService with actual @effect/cluster integration. This enables Entity sharding, persistent shard ownership via advisory locks, and cross-pod message routing.

Output: Fully functional `packages/server/src/infra/cluster.ts` with Entity.toLayer, SqlMessageStorage, SqlRunnerStorage, and ShardingConfig.
</objective>

<execution_context>
@/Users/bardiasamiee/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bardiasamiee/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-cluster-foundation/01-CONTEXT.md
@.planning/phases/01-cluster-foundation/01-RESEARCH.md
@.planning/phases/01-cluster-foundation/01-01-SUMMARY.md
@packages/server/src/infra/cluster.ts
@packages/server/src/context.ts
@packages/server/src/observe/telemetry.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Entity definition with handler and layer</name>
  <files>packages/server/src/infra/cluster.ts</files>
  <action>
Update `packages/server/src/infra/cluster.ts` to add the actual Entity definition with handler implementation.

**Add to imports:**
```typescript
import { Entity, Rpc, Sharding, SqlMessageStorage, SqlRunnerStorage, NodeClusterSocket, ShardingConfig } from '@effect/cluster';
import { PgClient } from '@effect/sql-pg';
import { Duration, Effect, Layer, Match, Schedule, Schema as S } from 'effect';
```

**Update [SCHEMA] section - add Entity definition:**
```typescript
// Entity message error schema
class EntityProcessError extends S.TaggedError<EntityProcessError>()('EntityProcessError', {
  message: S.String,
  cause: S.optional(S.Unknown),
}) {}

// Entity definition with polymorphic messages
const ClusterEntity = Entity.make('Cluster', [
  Rpc.make('process', {
    payload: ProcessPayload,
    success: S.Void,
    error: EntityProcessError,
  }),
  Rpc.make('status', {
    payload: StatusPayload,
    success: StatusResponse,
  }),
]);
```

**Add [ENTITY] section after SCHEMA:**
```typescript
// --- [ENTITY] ----------------------------------------------------------------

type EntityState = {
  status: 'idle' | 'processing' | 'complete' | 'failed';
  updatedAt: number;
};

const ClusterEntityLive = ClusterEntity.toLayer(Effect.gen(function* () {
  let state: EntityState = { status: 'idle', updatedAt: Date.now() };

  return {
    process: ({ data, idempotencyKey }) => Effect.gen(function* () {
      state = { status: 'processing', updatedAt: Date.now() };
      yield* Effect.logDebug('Entity processing', { idempotencyKey });
      // Handler implementation - extensible via handler registry
      state = { status: 'complete', updatedAt: Date.now() };
    }).pipe(
      Effect.catchAll((e) => Effect.fail(new EntityProcessError({
        message: e instanceof Error ? e.message : String(e),
        cause: e,
      }))),
    ),

    status: () => Effect.succeed({
      status: state.status,
      updatedAt: state.updatedAt,
    }),
  };
}), {
  maxIdleTime: _CONFIG.entity.maxIdleTime,
  concurrency: _CONFIG.entity.concurrency,
  mailboxCapacity: _CONFIG.entity.mailboxCapacity,  // Explicit capacity - prevents OOM (STATE.md blocker)
  defectRetryPolicy: Schedule.exponential(_CONFIG.retry.base).pipe(
    Schedule.jittered,
    Schedule.intersect(Schedule.recurs(_CONFIG.retry.maxAttempts)),
  ),
});
```

**Critical Requirements:**
- `mailboxCapacity: 100` explicit (not unbounded) - prevents OOM per STATE.md blocker
- `defectRetryPolicy` with exponential + jitter per research recommendations
- Handler logs via Effect.logDebug (integrates with existing telemetry)
  </action>
  <verify>
```bash
pnpm exec nx run server:typecheck
```
Check that mailboxCapacity is set:
```bash
grep -n "mailboxCapacity" packages/server/src/infra/cluster.ts
```
  </verify>
  <done>
- ClusterEntity defined with process and status RPCs
- ClusterEntityLive layer with explicit mailboxCapacity
- defectRetryPolicy with exponential + jitter
- EntityProcessError follows Schema.TaggedError pattern
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement storage layers with dedicated RunnerStorage connection</name>
  <files>packages/server/src/infra/cluster.ts</files>
  <action>
Add storage layer composition with dedicated connection for RunnerStorage.

**Add to [CONSTANTS] section:**
```typescript
const _RUNNER_STORAGE_CONFIG = {
  // Dedicated connection for advisory locks - MUST NOT share with app pool
  // See STATE.md: "Advisory locks require stable DB connections"
  poolSize: 1,  // Single dedicated connection
  idleTimeout: Duration.hours(1),  // Long-lived for advisory lock stability
} as const;
```

**Add [LAYERS] section before ENTRY_POINT:**
```typescript
// --- [LAYERS] ----------------------------------------------------------------

// Dedicated PgClient for RunnerStorage - prevents shard lock loss from connection recycling
// STATE.md blocker: "Advisory locks require stable DB connections"
const RunnerStorageClient = Layer.scoped(
  PgClient.PgClient,
  Effect.gen(function* () {
    // Uses separate config from main pool
    // Connection stays alive for advisory lock duration
    yield* Effect.logInfo('RunnerStorage: dedicated connection initialized');
    return yield* PgClient.make({
      // Config will be provided at composition root
    });
  }),
);

// SqlRunnerStorage with dedicated connection
const RunnerStorageLive = SqlRunnerStorage.layer.pipe(
  Layer.provide(RunnerStorageClient),
);

// SqlMessageStorage uses shared connection pool (OK for message persistence)
const MessageStorageLive = SqlMessageStorage.layer;

// ShardingConfig with K8s graceful shutdown
const ShardingConfigLive = ShardingConfig.layer({
  shardsPerGroup: 100,
  preemptiveShutdown: true,  // K8s graceful shutdown - STATE.md requirement
});

// NodeClusterSocket for pod-to-pod communication
const ClusterSocketLive = NodeClusterSocket.layer({ storage: 'sql' });

// Full cluster layer composition
const ClusterLive = Layer.mergeAll(
  ClusterEntityLive,
  ClusterSocketLive,
).pipe(
  Layer.provide(MessageStorageLive),
  Layer.provide(RunnerStorageLive),
  Layer.provide(ShardingConfigLive),
);
```

**Critical Requirements:**
- Dedicated connection for RunnerStorage (separate from app pool)
- `preemptiveShutdown: true` for K8s (prevents in-flight message loss)
- SqlMessageStorage uses shared pool (OK for message persistence)
- Layer composition follows existing codebase patterns
  </action>
  <verify>
```bash
pnpm exec nx run server:typecheck
```
Verify preemptiveShutdown is set:
```bash
grep -n "preemptiveShutdown" packages/server/src/infra/cluster.ts
```
Verify dedicated connection comment:
```bash
grep -n "dedicated connection" packages/server/src/infra/cluster.ts
```
  </verify>
  <done>
- RunnerStorageLive with dedicated PgClient
- MessageStorageLive with shared pool
- ShardingConfigLive with preemptiveShutdown: true
- ClusterSocketLive for pod communication
- ClusterLive combines all layers
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement ClusterService methods (send, broadcast, isLocal)</name>
  <files>packages/server/src/infra/cluster.ts</files>
  <action>
Update the ClusterService class with actual implementation using Sharding APIs.

**Update [SERVICES] section - replace stub with full implementation:**
```typescript
// --- [SERVICES] --------------------------------------------------------------

class ClusterService extends Effect.Service<ClusterService>()('server/Cluster', {
  effect: Effect.gen(function* () {
    const sharding = yield* Sharding.Sharding;
    yield* Effect.annotateLogsScoped({ 'service.name': 'cluster' });

    const send = <R>(entityId: string, request: R): Effect.Effect<void, ClusterError, never> =>
      sharding.send(ClusterEntity.make(entityId), request as never).pipe(
        Effect.mapError((e) => Match.value(e).pipe(
          Match.when({ _tag: 'MailboxFull' }, () => ClusterError.fromMailboxFull(entityId, e)),
          Match.when({ _tag: 'SendTimeout' }, () => ClusterError.fromSendTimeout(entityId, e)),
          Match.when({ _tag: 'EntityNotManagedByThisRunner' }, () => ClusterError.fromRunnerUnavailable(entityId, e)),
          Match.orElse(() => ClusterError.fromUnknown(e)),
        )),
        Effect.withSpan('cluster.send', { attributes: { 'entity.id': entityId } }),
      );

    const broadcast = (entityType: string, request: unknown): Effect.Effect<void, ClusterError, never> =>
      sharding.broadcastAll(entityType)(request as never).pipe(
        Effect.mapError((e) => ClusterError.fromUnknown(e)),
        Effect.withSpan('cluster.broadcast', { attributes: { 'entity.type': entityType } }),
      );

    const isLocal = (entityId: string): Effect.Effect<boolean, never, never> =>
      sharding.isEntityOnLocalRunner(entityId).pipe(
        Effect.withSpan('cluster.isLocal', { attributes: { 'entity.id': entityId } }),
      );

    yield* Effect.logInfo('ClusterService initialized');

    return { broadcast, isLocal, send };
  }),
  dependencies: [ClusterLive],
}) {}
```

**Update [ENTRY_POINT] section:**
```typescript
// --- [ENTRY_POINT] -----------------------------------------------------------

// biome-ignore lint/correctness/noUnusedVariables: const+namespace merge pattern
const ClusterService = {
  Default: ClusterService.Default,
  Error: ClusterError,
  errorTag: _errorTag,
  handleError: _handleClusterError,
  Layer: ClusterLive,
} as const;
```

**Update [NAMESPACE] section:**
```typescript
// --- [NAMESPACE] -------------------------------------------------------------

namespace ClusterService {
  export type Error = InstanceType<typeof ClusterError>;
  export type ErrorReason = Error['reason'];
  export type Config = typeof _CONFIG;
  export type Entity = typeof ClusterEntity;
}
```

**Critical Requirements:**
- Use `Effect.withSpan` for telemetry (integrates with existing observe/telemetry.ts)
- Map cluster-specific errors to ClusterError using Match (no instanceof)
- Expose ClusterLive layer for composition at app level
- Service follows Effect.Service pattern from codebase
  </action>
  <verify>
```bash
pnpm exec nx run server:typecheck
```
Verify Effect.withSpan is used:
```bash
grep -n "withSpan" packages/server/src/infra/cluster.ts
```
  </verify>
  <done>
- ClusterService.send routes messages to entities
- ClusterService.broadcast sends to all entities of type
- ClusterService.isLocal checks shard locality
- All methods have telemetry spans
- Error mapping uses Match.value (no instanceof)
  </done>
</task>

</tasks>

<verification>
Overall verification for Plan 02:

1. **Full typecheck:**
   ```bash
   pnpm exec nx run server:typecheck
   ```

2. **Required configurations present:**
   ```bash
   grep -n "preemptiveShutdown: true" packages/server/src/infra/cluster.ts
   grep -n "mailboxCapacity" packages/server/src/infra/cluster.ts
   grep -n "dedicated connection" packages/server/src/infra/cluster.ts
   ```

3. **No forbidden patterns:**
   ```bash
   grep -n "SELECT FOR UPDATE" packages/server/src/infra/cluster.ts && echo "FAIL" || echo "PASS: no DB polling"
   grep -n "instanceof" packages/server/src/infra/cluster.ts && echo "FAIL" || echo "PASS: no instanceof"
   ```

4. **File size check:**
   ```bash
   wc -l packages/server/src/infra/cluster.ts  # Must be under 225
   ```

5. **Layer composition correct:**
   ```bash
   grep -n "Layer.provide" packages/server/src/infra/cluster.ts
   ```
</verification>

<success_criteria>
- [ ] ClusterEntity defined with toLayer and explicit mailboxCapacity
- [ ] RunnerStorageLive uses dedicated PgClient connection
- [ ] MessageStorageLive uses shared pool
- [ ] ShardingConfig has preemptiveShutdown: true
- [ ] ClusterService.send/broadcast/isLocal implemented
- [ ] All methods have Effect.withSpan telemetry
- [ ] Error mapping uses Match.value (no instanceof)
- [ ] File under 225 LOC
- [ ] Full typecheck passes
</success_criteria>

<output>
After completion, create `.planning/phases/01-cluster-foundation/01-02-SUMMARY.md`
</output>
