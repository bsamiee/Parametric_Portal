---
phase: 01-cluster-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - packages/server/src/infra/cluster.ts
autonomous: true

must_haves:
  truths:
    - "Entity message sent on Pod A reaches handler on Pod B within 100ms (Effect.timeout enforced)"
    - "Shard ownership persists across pod restarts without message loss"
    - "Work claims via shard ownership without SELECT FOR UPDATE patterns"
    - "preemptiveShutdown: true configured for K8s graceful shutdown"
    - "Entity handlers implement idempotent pattern via Rpc.make({ primaryKey }) (automatic deduplication)"
    - "PgClient for RunnerStorage uses maxSize:1/minSize:1 with 24h TTL for advisory lock stability"
    - "Snowflake.layerGenerator provides cluster-aware unique ID generation"
    - "sharding.messenger used for type-safe entity client invocation"
    - "Effect.catchTags for idiomatic error mapping (not mapError + Match)"
    - "Effect.timeoutFail combines timeout + error mapping (not timeout + catchTag)"
    - "Schedule.upTo caps total retry duration to prevent infinite retry storms"
    - "EntityState as Schema.Class with pendingSignal for Phase 6 DurableDeferred compatibility"
    - "Entity handlers preserve Cause structure for Workflow.withCompensation"
    - "KeyValueStore.layerSchema provides entity state snapshots for crash recovery (EVNT-05)"
    - "HttpTraceContext propagates W3C trace context across cluster pods"
    - "Cluster uses internal serialization; RpcSerialization.layerMsgPack deferred to Phase 7"
    - "Rpc.make({ primaryKey }) enables schema-level idempotency (automatic deduplication)"
  artifacts:
    - path: "packages/server/src/infra/cluster.ts"
      provides: "Full ClusterService with Entity layer, storage layers, and sharding config"
      exports: ["ClusterService"]
      max_lines: 225
  key_links:
    - from: "packages/server/src/infra/cluster.ts"
      to: "@effect/cluster SqlRunnerStorage"
      via: "Layer.provide with dedicated PgClient (maxSize:1, long TTL)"
      pattern: "SqlRunnerStorage\\.layer"
    - from: "packages/server/src/infra/cluster.ts"
      to: "@effect/cluster NodeClusterSocket"
      via: "Layer composition"
      pattern: "NodeClusterSocket\\.layer"
    - from: "packages/server/src/infra/cluster.ts"
      to: "@effect/cluster Snowflake"
      via: "Snowflake.layerGenerator for ID generation"
      pattern: "Snowflake\\.layerGenerator"
    - from: "packages/server/src/infra/cluster.ts"
      to: "@effect/cluster RunnerHealth"
      via: "RunnerHealth.layerK8s for K8s pod health"
      pattern: "RunnerHealth\\.layerK8s"
    - from: "packages/server/src/infra/cluster.ts (Entity schema)"
      to: "@effect/rpc Rpc.make"
      via: "primaryKey function for automatic idempotency deduplication"
      pattern: "primaryKey:"
---

<objective>
Implement Entity layer, SQL storage backends, and cluster configuration for multi-pod coordination.

Purpose: Complete the ClusterService with actual @effect/cluster integration. This enables Entity sharding, persistent shard ownership via advisory locks, and cross-pod message routing.

Output: Fully functional `packages/server/src/infra/cluster.ts` with Entity.toLayer, SqlMessageStorage, SqlRunnerStorage, and ShardingConfig.
</objective>

<execution_context>
@/Users/bardiasamiee/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bardiasamiee/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-cluster-foundation/01-CONTEXT.md
@.planning/phases/01-cluster-foundation/01-RESEARCH.md
@.planning/phases/01-cluster-foundation/01-01-SUMMARY.md
@packages/server/src/infra/cluster.ts
@packages/server/src/context.ts
@packages/server/src/observe/telemetry.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Entity definition with handler and layer</name>
  <files>packages/server/src/infra/cluster.ts</files>
  <action>
Update `packages/server/src/infra/cluster.ts` to add the actual Entity definition with handler implementation.

**Add to imports:**
```typescript
import { Entity, Rpc, Sharding, SqlMessageStorage, SqlRunnerStorage, NodeClusterSocket, ShardingConfig, MessageStorage, Snowflake, RunnerHealth } from '@effect/cluster';
import { HttpTraceContext, KeyValueStore, MsgPack } from '@effect/platform';
import { PgClient } from '@effect/sql-pg';
import { Chunk, Duration, Effect, Layer, Match, Option, Ref, Schedule, Schema as S, Cause, Config, Stream } from 'effect';
import { Client as DbClient } from '@parametric/database/client';
import { Telemetry } from '../observe/telemetry.ts';
```

**Update [SCHEMA] section - add Entity definition:**
```typescript
// Entity message error schema
class EntityProcessError extends S.TaggedError<EntityProcessError>()('EntityProcessError', {
  message: S.String,
  cause: S.optional(S.Unknown),
}) {}

// Entity definition with polymorphic messages
// (#R1) primaryKey enables automatic idempotency via MessageStorage.saveRequest
const ClusterEntity = Entity.make('Cluster', [
  Rpc.make('process', {
    payload: ProcessPayload,
    success: S.Void,
    error: EntityProcessError,
    // primaryKey moves idempotency to schema level - reduces handler complexity
    primaryKey: (p) => p.idempotencyKey ?? `${Snowflake.toString(p.entityId)}:${Date.now()}`,
  }),
  Rpc.make('status', {
    payload: StatusPayload,
    success: StatusResponse,
    // No primaryKey - status queries are naturally idempotent
  }),
]);
```

**Add [ENTITY] section after SCHEMA:**
```typescript
// --- [ENTITY] ----------------------------------------------------------------

// Entity state as Schema.Class for DurableDeferred compatibility (Phase 6 workflow adoption)
class EntityState extends S.Class<EntityState>('EntityState')({
  status: S.Literal('idle', 'processing', 'suspended', 'complete', 'failed'),
  updatedAt: S.Number,
  // Pre-wire for DurableDeferred token (workflow integration Phase 6)
  pendingSignal: S.optional(S.Struct({
    name: S.String,      // DurableDeferred name
    token: S.String,     // DurableDeferred.Token
  })),
}) {
  static readonly idle = () => new EntityState({ status: 'idle', updatedAt: Date.now() })
  static readonly processing = () => new EntityState({ status: 'processing', updatedAt: Date.now() })
  static readonly suspended = (signal: { name: string; token: string }) =>
    new EntityState({ status: 'suspended', updatedAt: Date.now(), pendingSignal: signal })
}

const ClusterEntityLive = ClusterEntity.toLayer(Effect.gen(function* () {
  const storage = yield* MessageStorage.MessageStorage;
  // Access entity identity for state correlation (#5)
  const currentAddress = yield* Entity.CurrentAddress;
  // Use Ref for typed state management (NEVER let/var per CLAUDE.md)
  const stateRef = yield* Ref.make(EntityState.idle());

  return {
    // (#R7) Rpc.make primaryKey handles idempotency automatically via MessageStorage
    // Manual requestIdForPrimaryKey check is redundant - MessageStorage.saveRequest
    // returns Duplicate with lastReceivedReply for cached responses
    process: ({ data }) => Effect.gen(function* () {
      yield* Ref.set(stateRef, EntityState.processing());
      yield* Effect.logDebug('Entity processing', { idempotencyKey });
      // Handler implementation - extensible via handler registry
      yield* Ref.set(stateRef, new EntityState({ status: 'complete', updatedAt: Date.now() }));
    }).pipe(
      // Use Effect.ensuring for cleanup guarantee regardless of outcome
      Effect.ensuring(Ref.update(stateRef, (s) => ({ ...s, updatedAt: Date.now() }))),
      // Distinguish failures (expected) from defects (unexpected) for better error handling
      // NOTE: Preserve original Cause for Phase 6 Workflow.withCompensation compatibility
      // Compensation callback receives (value, cause) - don't squash typed errors
      Effect.catchAllCause((cause) => {
        const failures = Cause.failures(cause);
        const defects = Cause.defects(cause);
        const isDefect = Chunk.isNonEmpty(defects);

        return Effect.fail(new EntityProcessError({
          message: isDefect ? 'Internal error' : Cause.pretty(cause),
          cause,  // Preserve full Cause structure for workflow compensation
        }));
      }),
    ),

    status: () => Ref.get(stateRef).pipe(
      Effect.map((s) => ({ status: s.status, updatedAt: s.updatedAt })),
    ),
  };
}), {
  maxIdleTime: _CONFIG.entity.maxIdleTime,
  concurrency: _CONFIG.entity.concurrency,
  mailboxCapacity: _CONFIG.entity.mailboxCapacity,  // Explicit capacity - prevents OOM (STATE.md blocker)
  // Retry policy aligned with Activity.retry({ times: N, schedule }) for Phase 6 workflow wrapping
  // Schedule.upTo caps total retry duration to prevent infinite retry storms
  defectRetryPolicy: Schedule.exponential(_CONFIG.retry.defect.base).pipe(
    Schedule.jittered,
    Schedule.intersect(Schedule.recurs(_CONFIG.retry.defect.maxAttempts)),
    Schedule.upTo(Duration.seconds(30)),  // Total duration cap
  ),
  // (#6) Control whether defects should be retried or immediately fail
  disableFatalDefects: false,  // Allow defect retry policy to work
  // (#7) Custom tracing metadata on entity operations
  spanAttributes: {
    'entity.service': 'cluster-infrastructure',
    'entity.version': 'v1',
  },
});
```

**Critical Requirements:**
- `mailboxCapacity: 100` explicit (not unbounded) - prevents OOM per STATE.md blocker
- `defectRetryPolicy` with exponential + jitter per research recommendations
- `disableFatalDefects: false` to enable defect recovery
- `spanAttributes` for custom tracing metadata
- Handler logs via Effect.logDebug (integrates with existing telemetry)
- Idempotency via `Rpc.make({ primaryKey })` - automatic deduplication at schema level
  </action>
  <verify>
```bash
pnpm exec nx run server:typecheck
```
Check that mailboxCapacity is set:
```bash
grep -n "mailboxCapacity" packages/server/src/infra/cluster.ts
```
Check that primaryKey is defined for idempotency:
```bash
grep -n "primaryKey:" packages/server/src/infra/cluster.ts
```
  </verify>
  <done>
- ClusterEntity defined with process and status RPCs
- ClusterEntityLive layer with explicit mailboxCapacity
- defectRetryPolicy with exponential + jitter
- EntityProcessError follows Schema.TaggedError pattern
- Idempotent handler via Rpc.make primaryKey (automatic deduplication)
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement storage layers with dedicated RunnerStorage connection</name>
  <files>packages/server/src/infra/cluster.ts</files>
  <action>
Add storage layer composition with dedicated connection for RunnerStorage.

**Add to [CONSTANTS] section:**
```typescript
const _RUNNER_STORAGE_CONFIG = {
  // Dedicated connection for advisory locks - MUST NOT share with app pool
  // See STATE.md: "Advisory locks require stable DB connections"
  poolSize: 1,  // Single dedicated connection
  idleTimeout: Duration.hours(1),  // Long-lived for advisory lock stability
} as const;
```

**Add [LAYERS] section before ENTRY_POINT:**
```typescript
// --- [LAYERS] ----------------------------------------------------------------

// Dedicated PgClient for RunnerStorage - prevents shard lock loss from connection recycling
// STATE.md blocker: "Advisory locks require stable DB connections"
// Research finding: Single dedicated connection with long TTL for lock stability
// (#9) Use correct PgClient config names: maxSize/minSize (not maxConnections/minConnections)
const RunnerStoragePgClient = PgClient.layerConfig({
  // Reuse connection params from environment
  host: Config.string('POSTGRES_HOST').pipe(Config.withDefault('localhost')),
  port: Config.integer('POSTGRES_PORT').pipe(Config.withDefault(5432)),
  database: Config.string('POSTGRES_DB').pipe(Config.withDefault('parametric')),
  username: Config.string('POSTGRES_USER').pipe(Config.withDefault('postgres')),
  password: Config.redacted('POSTGRES_PASSWORD'),

  // Advisory lock-specific configuration (research findings)
  applicationName: Config.succeed('cluster-runner-storage'),
  maxSize: Config.succeed(1),             // Single dedicated connection
  minSize: Config.succeed(1),             // Keep alive always
  idleTimeout: Config.succeed(Duration.hours(24)),     // Never idle timeout
  timeToLive: Config.succeed(Duration.hours(24)),      // Long-lived connection
  acquireTimeout: Config.succeed(Duration.seconds(10)),

  // Observability
  spanAttributes: Config.succeed({
    'db.system': 'postgresql',
    'service.name': 'cluster-runner-storage',
  }),
});

// SqlRunnerStorage with dedicated connection (CANNOT use shared DbClient.layer)
const RunnerStorageLive = SqlRunnerStorage.layer.pipe(
  Layer.provide(RunnerStoragePgClient),
);

// SqlMessageStorage uses shared connection pool via existing DbClient.layer
// (OK for message persistence - no advisory lock requirement)
const MessageStorageLive = SqlMessageStorage.layer.pipe(
  Layer.provide(DbClient.layer),  // Reference existing client.ts layer
);

// (#F1) Snowflake ID generator for cluster-aware unique IDs
const SnowflakeLive = Snowflake.layerGenerator;

// (#P3) KeyValueStore for entity state snapshots - crash recovery per EVNT-05
const EntityStateStoreLive = KeyValueStore.layerSchema(EntityState, {
  prefix: 'cluster:entity:state:',
});

// NOTE: MsgPack serialization is handled internally by cluster messaging
// Phase 7 will add RpcSerialization.layerMsgPack for WebSocket RPC

// ShardingConfig with K8s graceful shutdown
const ShardingConfigLive = ShardingConfig.layer({
  shardsPerGroup: 100,
  preemptiveShutdown: true,  // K8s graceful shutdown - STATE.md requirement
});

// (#10) NodeClusterSocket for pod-to-pod communication
// IMPORTANT: Verify actual API in node_modules before implementation
// NodeClusterSocket moved from @effect/platform-node to @effect/cluster in Effect 3.19
// Storage configuration happens via Layer.provide with SqlRunnerStorage/SqlMessageStorage
const ClusterSocketLive = NodeClusterSocket.layer;

// (#F6) RunnerHealth for K8s production deployment
// Namespace configurable via K8S_NAMESPACE env var for multi-namespace deployments
// NOTE: Use Layer.unwrapEffect with Effect.gen (not Config.pipe + Effect.map)
const RunnerHealthLive = Layer.unwrapEffect(
  Effect.gen(function* () {
    const namespace = yield* Config.string('K8S_NAMESPACE').pipe(Config.withDefault('default'));
    return RunnerHealth.layerK8s({
      namespace,
      labelSelector: 'app=parametric-portal',
    });
  }),
);
// Development: use RunnerHealth.layerNoop or RunnerHealth.layerPing

// Layer dependency order:
// ClusterLive
//   └── ClusterEntityLive, NodeClusterSocket, SnowflakeLive, RunnerHealthLive
//       └── Sharding (internal)
//           └── MessageStorageLive, RunnerStorageLive, ShardingConfigLive
//               └── PgClient (shared for MessageStorage, dedicated for RunnerStorage)
const ClusterLive = Layer.mergeAll(
  ClusterEntityLive,
  ClusterSocketLive,
  SnowflakeLive,          // (#F1) Snowflake ID generation
  RunnerHealthLive,       // (#F6) K8s health checking
  EntityStateStoreLive,   // (#P3) Entity state snapshots
).pipe(
  Layer.provide(MessageStorageLive),
  Layer.provide(RunnerStorageLive),
  Layer.provide(ShardingConfigLive),
);
```

**Critical Requirements:**
- Dedicated connection for RunnerStorage (separate from app pool)
- `preemptiveShutdown: true` for K8s (prevents in-flight message loss)
- SqlMessageStorage uses shared pool (OK for message persistence)
- Snowflake.layerGenerator for cluster-aware ID generation
- RunnerHealth.layerK8s for K8s health checking
- Layer composition follows existing codebase patterns
  </action>
  <verify>
```bash
pnpm exec nx run server:typecheck
```
Verify preemptiveShutdown is set:
```bash
grep -n "preemptiveShutdown" packages/server/src/infra/cluster.ts
```
Verify dedicated connection comment:
```bash
grep -n "dedicated connection" packages/server/src/infra/cluster.ts
```
  </verify>
  <done>
- RunnerStorageLive with dedicated PgClient
- MessageStorageLive with shared pool
- ShardingConfigLive with preemptiveShutdown: true
- ClusterSocketLive for pod communication
- ClusterLive combines all layers
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement ClusterService methods (send, broadcast, isLocal)</name>
  <files>packages/server/src/infra/cluster.ts</files>
  <action>
Update the ClusterService class with actual implementation using Sharding APIs.

**Update [SERVICES] section - replace stub with full implementation:**
```typescript
// --- [SERVICES] --------------------------------------------------------------

// Retry policy for transient cluster errors (MailboxFull, timeout)
// Schedule.upTo caps total retry duration to prevent infinite retry storms
const _transientRetryPolicy = Schedule.exponential(Duration.millis(50)).pipe(
  Schedule.intersect(Schedule.recurs(3)),
  Schedule.jittered,
  Schedule.upTo(Duration.seconds(5)),  // Total duration cap for transient retries
);

// Check if error is transient (retryable)
const _isTransientError = (e: ClusterError): boolean =>
  e.reason === 'MailboxFull' || e.reason === 'SendTimeout';

class ClusterService extends Effect.Service<ClusterService>()('server/Cluster', {
  effect: Effect.gen(function* () {
    const sharding = yield* Sharding.Sharding;
    yield* Effect.annotateLogsScoped({ 'service.name': 'cluster' });

    // (#11) Use sharding.messenger for type-safe entity clients
    const client = sharding.messenger(ClusterEntity);

    // Use Telemetry.span for auto context/metrics/error capture (codebase pattern)
    // Use Effect.catchTags for idiomatic error handling (not mapError + Match)
    // Use Effect.timeoutFail to enforce 100ms SLA (combines timeout + error mapping)
    // Use Effect.retry for transient errors
    // (#P4) HttpTraceContext for distributed trace propagation across pods
    const send = <R>(entityId: string, request: R): Effect.Effect<void, ClusterError, never> =>
      Telemetry.span(
        Effect.gen(function* () {
          // Propagate trace context across cluster pods via W3C Trace Context
          const span = yield* Effect.currentSpan.pipe(Effect.orElseSucceed(() => undefined));
          const traceHeaders = span ? HttpTraceContext.toHeaders(span) : {};
          const enrichedRequest = { ...request as object, _trace: traceHeaders };

          // (#11) Type-safe entity client invocation with trace context
          return yield* client(entityId).process(enrichedRequest as never);
        }).pipe(
          // Effect.timeoutFail combines timeout + error mapping (cleaner than timeout + catchTag)
          Effect.timeoutFail({
            onTimeout: () => ClusterError.fromSendTimeout(entityId, 'SLA exceeded'),
            duration: Duration.millis(100),  // Enforce 100ms SLA
          }),
          // (#12) EntityNotAssignedToRunner replaces EntityNotManagedByThisRunner in 3.19
          Effect.catchTags({
            MailboxFull: (e) => Effect.fail(ClusterError.fromMailboxFull(entityId, e)),
            SendTimeout: (e) => Effect.fail(ClusterError.fromSendTimeout(entityId, e)),
            EntityNotAssignedToRunner: (e) => Effect.fail(ClusterError.fromEntityNotAssigned(entityId, e)),
            AlreadyProcessingMessage: (e) => Effect.fail(ClusterError.fromAlreadyProcessing(entityId, e)),
            MalformedMessage: (e) => Effect.fail(ClusterError.fromMalformedMessage(e)),
            PersistenceError: (e) => Effect.fail(ClusterError.fromPersistence(e)),
            RunnerNotRegistered: (e) => Effect.fail(ClusterError.fromRunnerNotRegistered(e)),
          }),
          Effect.catchAll((e) => Effect.fail(ClusterError.fromRunnerUnavailable(entityId, e))),
          Effect.retry(Schedule.recurWhile(_isTransientError).pipe(
            Schedule.intersect(_transientRetryPolicy),
          )),
        ),
        'cluster.send',
        { 'entity.id': entityId },
      );

    // (#13) Use sharding.broadcast (not broadcastAll)
    const broadcast = (request: unknown): Effect.Effect<void, ClusterError, never> =>
      Telemetry.span(
        sharding.broadcast(ClusterEntity)(request as never).pipe(
          Effect.catchAll((e) => Effect.fail(ClusterError.fromRunnerUnavailable('broadcast', e))),
        ),
        'cluster.broadcast',
      );

    // (#14) isEntityOnLocalRunner needs Entity type, not just ID
    const isLocal = (entityId: string): Effect.Effect<boolean, never, never> =>
      Telemetry.span(
        sharding.isEntityOnLocalRunner(ClusterEntity, entityId),
        'cluster.isLocal',
        { 'entity.id': entityId },
      );

    // (#F1) Expose ID generation via Snowflake.Generator
    const generateId = Effect.gen(function* () {
      const generator = yield* Snowflake.Generator;
      return generator.unsafeNext();
    });

    yield* Effect.logInfo('ClusterService initialized');

    return { broadcast, generateId, isLocal, send };
  }),
  dependencies: [ClusterLive],
}) {}
```

**Update [ENTRY_POINT] section:**
```typescript
// --- [ENTRY_POINT] -----------------------------------------------------------

// biome-ignore lint/correctness/noUnusedVariables: const+namespace merge pattern
const ClusterService = {
  Default: ClusterService.Default,
  Error: ClusterError,
  errorTag: _errorTag,
  handleError: _handleClusterError,
  Layer: ClusterLive,
} as const;
```

**Update [NAMESPACE] section:**
```typescript
// --- [NAMESPACE] -------------------------------------------------------------

namespace ClusterService {
  export type Error = InstanceType<typeof ClusterError>;
  export type ErrorReason = Error['reason'];
  export type Config = typeof _CONFIG;
  export type Entity = typeof ClusterEntity;
}
```

**Critical Requirements:**
- Use `Effect.withSpan` for telemetry (integrates with existing observe/telemetry.ts)
- Map cluster-specific errors to ClusterError using Match (no instanceof)
- Expose ClusterLive layer for composition at app level
- Service follows Effect.Service pattern from codebase
  </action>
  <verify>
```bash
pnpm exec nx run server:typecheck
```
Verify Effect.withSpan is used:
```bash
grep -n "withSpan" packages/server/src/infra/cluster.ts
```
  </verify>
  <done>
- ClusterService.send routes messages to entities
- ClusterService.broadcast sends to all entities of type
- ClusterService.isLocal checks shard locality
- All methods have telemetry spans
- Error mapping uses Match.value (no instanceof)
  </done>
</task>

</tasks>

<verification>
Overall verification for Plan 02:

1. **Full typecheck:**
   ```bash
   pnpm exec nx run server:typecheck
   ```

2. **Required configurations present:**
   ```bash
   grep -n "preemptiveShutdown: true" packages/server/src/infra/cluster.ts
   grep -n "mailboxCapacity" packages/server/src/infra/cluster.ts
   grep -n "dedicated connection" packages/server/src/infra/cluster.ts
   ```

3. **Idempotency pattern (Success Criterion 6):**
   ```bash
   grep -n "primaryKey:" packages/server/src/infra/cluster.ts
   # Note: MessageStorage.saveRequest is called internally by cluster - handler doesn't need explicit call
   ```

4. **No forbidden patterns:**
   ```bash
   grep -n "SELECT FOR UPDATE" packages/server/src/infra/cluster.ts && echo "FAIL" || echo "PASS: no DB polling"
   grep -n "instanceof" packages/server/src/infra/cluster.ts && echo "FAIL" || echo "PASS: no instanceof"
   grep -n "let state" packages/server/src/infra/cluster.ts && echo "FAIL" || echo "PASS: no mutable state"
   grep -n "if.*idempotencyKey" packages/server/src/infra/cluster.ts && echo "FAIL" || echo "PASS: no if chains"
   ```

5. **Ref.make and DbClient.layer used:**
   ```bash
   grep -n "Ref.make" packages/server/src/infra/cluster.ts
   grep -n "DbClient.layer" packages/server/src/infra/cluster.ts
   ```

6. **Effect patterns used correctly:**
   ```bash
   grep -n "Telemetry.span" packages/server/src/infra/cluster.ts  # Not Effect.withSpan
   grep -n "Effect.timeout" packages/server/src/infra/cluster.ts  # SLA enforcement
   grep -n "Effect.catchTags" packages/server/src/infra/cluster.ts  # Not mapError + Match
   grep -n "Effect.retry" packages/server/src/infra/cluster.ts  # Transient error retry
   grep -n "Effect.if" packages/server/src/infra/cluster.ts  # Not Option.match
   grep -n "Effect.ensuring" packages/server/src/infra/cluster.ts  # Cleanup guarantee
   ```

7. **File size check:**
   ```bash
   wc -l packages/server/src/infra/cluster.ts  # Must be under 225
   ```

8. **Layer composition correct:**
   ```bash
   grep -n "Layer.provide" packages/server/src/infra/cluster.ts
   ```
</verification>

<success_criteria>
- [ ] ClusterEntity defined with toLayer and explicit mailboxCapacity
- [ ] Entity handler uses Ref.make for state (no mutable let/var)
- [ ] Entity schema uses Rpc.make({ primaryKey }) for idempotency (Success Criterion 6)
- [ ] Idempotency check uses Effect.if (not Option.match or if/else chains)
- [ ] Error handling uses Cause.failures/Cause.defects distinction
- [ ] Entity handler uses Effect.ensuring for cleanup guarantee
- [ ] RunnerStorageLive uses dedicated PgClient with full config (max/min 1, long TTL)
- [ ] MessageStorageLive uses existing DbClient.layer (not hand-rolled)
- [ ] ShardingConfig has preemptiveShutdown: true
- [ ] ClusterService.send/broadcast/isLocal implemented
- [ ] All methods use Telemetry.span (not Effect.withSpan)
- [ ] ClusterService.send uses Effect.timeout for 100ms SLA enforcement
- [ ] ClusterService.send uses Effect.catchTags (not mapError + Match)
- [ ] ClusterService.send uses Effect.retry for transient errors
- [ ] File under 225 LOC
- [ ] Full typecheck passes
</success_criteria>

<output>
After completion, create `.planning/phases/01-cluster-foundation/01-02-SUMMARY.md`
</output>
