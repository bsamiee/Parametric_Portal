---
phase: 04-job-processing
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - packages/server/src/infra/jobs.ts
autonomous: true

must_haves:
  truths:
    - "Job submission to processing latency under 50ms"
    - "JobService interface unchanged for existing callers (enqueue, registerHandler)"
    - "Failed jobs appear in job_dlq table after max retries"
    - "Critical priority jobs process before low priority jobs under load"
    - "Long-duration jobs remain alive beyond 5min idle timeout"
    - "Progress streaming available via progress RPC"
    - "JobContext provides jobId/tenantId/priority to handlers"
    - "validateBatch collects ALL validation errors (no fail-fast)"
    - "Single unified export: JobService only (const+namespace merge pattern)"
    - "Cancel handler interrupts fiber and increments cancellations metric"
    - "DLQ insert uses tenant from Context.Request.tenantId"
  artifacts:
    - path: "packages/server/src/infra/jobs.ts"
      provides: "JobService with Entity-based dispatch"
      contains: "Entity.make"
      min_lines: 100
  key_links:
    - from: "packages/server/src/infra/jobs.ts"
      to: "packages/server/src/infra/cluster.ts"
      via: "ClusterService Layer composition"
      pattern: "Layer\\.provide.*ClusterService"
    - from: "packages/server/src/infra/jobs.ts"
      to: "packages/server/src/observe/telemetry.ts"
      via: "Telemetry.span for job operations"
      pattern: "Telemetry\\.span"
    - from: "packages/server/src/infra/jobs.ts"
      to: "@parametric-portal/database/repos"
      via: "DatabaseService.jobDlq"
      pattern: "db\\.jobDlq"

success_criteria_detail:
  - id: SC-01
    criterion: "No poll loop or SELECT FOR UPDATE in jobs.ts"
    verify: "grep -E 'FOR UPDATE|poll' packages/server/src/infra/jobs.ts returns empty"
  - id: SC-02
    criterion: "File under 275 LOC with const + namespace merge pattern"
    verify: "wc -l packages/server/src/infra/jobs.ts < 275"
  - id: SC-03
    criterion: "Single export: JobService only"
    verify: "grep '^export' packages/server/src/infra/jobs.ts shows only JobService"
  - id: SC-04
    criterion: "Priority weighted scheduling via pool sizing (4:3:2:1 ratio)"
    verify: "grep '_CONFIG.pools' jobs.ts shows { critical: 4, high: 3, normal: 2, low: 1 }"
  - id: SC-05
    criterion: "Entity.keepAlive for long-duration jobs"
    verify: "grep 'keepAlive' jobs.ts shows toggling based on duration === 'long'"
  - id: SC-06
    criterion: "Schedule.collectAllInputs accumulates retry errors for DLQ"
    verify: "grep 'collectAllInputs' jobs.ts returns line in _CONFIG.retry"
  - id: SC-07
    criterion: "progress RPC with stream: true for progress streaming"
    verify: "grep -A2 'progress' jobs.ts shows stream: true"
  - id: SC-08
    criterion: "validateBatch uses Effect.all mode: 'validate'"
    verify: "grep 'mode.*validate' jobs.ts returns line"
  - id: SC-09
    criterion: "Cancel handler calls Fiber.interrupt"
    verify: "grep 'Fiber.interrupt' jobs.ts returns line"
  - id: SC-10
    criterion: "Cancel handler increments cancellations metric"
    verify: "grep 'jobs.cancellations' jobs.ts returns line in cancel handler"
  - id: SC-11
    criterion: "DLQ uses Context.Request.tenantId for appId"
    verify: "grep 'Context.Request.tenantId' jobs.ts appears before db.jobDlq.insert"
  - id: SC-12
    criterion: "Telemetry.span wraps processJob"
    verify: "grep 'Telemetry.span' jobs.ts returns line"
  - id: SC-13
    criterion: "reportProgress method exposes progress queue"
    verify: "grep 'reportProgress' jobs.ts returns line"
  - id: SC-14
    criterion: "Static properties for values, namespace for types only (matching cluster.ts)"
    verify: "grep 'static readonly Context' jobs.ts returns line; grep 'export const' jobs.ts returns empty"
---

<objective>
Replace poll-based job queue with Entity mailbox dispatch.

Purpose: Eliminate polling latency and DB contention via @effect/cluster Entity pattern. Jobs dispatch instantly via consistent-hash routing to entity mailboxes.
Output: Completely rewritten jobs.ts with JobEntity, JobService facade, preserving existing API.
</objective>

<execution_context>
@/Users/bardiasamiee/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bardiasamiee/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-job-processing/04-RESEARCH.md
@.planning/phases/04-job-processing/04-01-SUMMARY.md
@packages/server/src/infra/jobs.ts
@packages/server/src/infra/cluster.ts
@packages/server/src/observe/metrics.ts
@packages/server/src/observe/telemetry.ts
@packages/server/src/context.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Imports and schema definitions</name>
  <files>packages/server/src/infra/jobs.ts</files>
  <action>
Replace the entire jobs.ts file. Start with imports and schemas.

**Imports (COMPLETE LIST - includes Clock, Layer, Stream, Queue, Fiber):**
```typescript
/**
 * Entity-based job processing via @effect/cluster mailbox dispatch.
 * Replaces poll-based queue with instant consistent-hash routing.
 */
import { Entity, Rpc, Sharding } from '@effect/cluster';
import { Clock, Data, Duration, Effect, Fiber, FiberMap, HashMap, Layer, Metric, Option, Queue, Ref, Schedule, Schema as S, Stream } from 'effect';
import { DatabaseService } from '@parametric-portal/database/repos';
import { Context } from '../context.ts';
import { MetricsService } from '../observe/metrics.ts';
import { Telemetry } from '../observe/telemetry.ts';
import { ClusterService } from './cluster.ts';
```

**Schema Section:**
```typescript
// --- [SCHEMA] ----------------------------------------------------------------

const JobPriority = S.Literal('critical', 'high', 'normal', 'low');
const JobStatus = S.Literal('queued', 'processing', 'complete', 'failed', 'cancelled');

class JobPayload extends S.Class<JobPayload>('JobPayload')({
  batchId: S.optional(S.String),
  dedupeKey: S.optional(S.String),
  duration: S.optional(S.Literal('short', 'long'), { default: () => 'short' as const }),
  maxAttempts: S.optional(S.Number, { default: () => 3 }),
  payload: S.Unknown,
  priority: S.optional(JobPriority, { default: () => 'normal' as const }),
  type: S.String,
}) {}

class JobStatusResponse extends S.Class<JobStatusResponse>('JobStatusResponse')({
  attempts: S.Number,
  history: S.Array(S.Struct({ error: S.optional(S.String), status: JobStatus, timestamp: S.Number })),
  result: S.optional(S.Unknown),
  status: JobStatus,
}) {}
```

**Errors Section:**
```typescript
// --- [ERRORS] ----------------------------------------------------------------

class JobError extends Data.TaggedError('JobError')<{
  readonly cause?: unknown;
  readonly jobId?: string;
  readonly reason: 'NotFound' | 'AlreadyCancelled' | 'HandlerMissing' | 'Validation' | 'Processing' | 'MaxRetries' | 'RunnerUnavailable' | 'Timeout';
}> {
  static readonly fromNotFound = (jobId: string) => new JobError({ jobId, reason: 'NotFound' });
  static readonly fromCancelled = (jobId: string) => new JobError({ jobId, reason: 'AlreadyCancelled' });
  static readonly fromHandlerMissing = (jobId: string, type: string) => new JobError({ cause: { type }, jobId, reason: 'HandlerMissing' });
  static readonly fromValidation = (jobId: string, cause: unknown) => new JobError({ cause, jobId, reason: 'Validation' });
  static readonly fromProcessing = (jobId: string, cause: unknown) => new JobError({ cause, jobId, reason: 'Processing' });
  static readonly fromMaxRetries = (jobId: string, cause: unknown) => new JobError({ cause, jobId, reason: 'MaxRetries' });
  static readonly fromRunnerUnavailable = (jobId: string, cause?: unknown) => new JobError({ cause, jobId, reason: 'RunnerUnavailable' });
  static readonly fromTimeout = (jobId: string, cause?: unknown) => new JobError({ cause, jobId, reason: 'Timeout' });
  static readonly _terminal: ReadonlySet<JobError['reason']> = new Set(['Validation', 'HandlerMissing', 'AlreadyCancelled', 'NotFound']);
  static readonly isTerminal = (e: JobError): boolean => JobError._terminal.has(e.reason);
}
```

**State Section:**
```typescript
class JobState extends S.Class<JobState>('JobState')({
  attempts: S.Number,
  completedAt: S.optional(S.Number),
  createdAt: S.Number,
  lastError: S.optional(S.String),
  result: S.optional(S.Unknown),
  status: JobStatus,
}) {
  static readonly queued = (ts: number) => new JobState({ attempts: 0, createdAt: ts, status: 'queued' });
  static readonly processing = (state: JobState) => new JobState({ ...state, status: 'processing' });
  static readonly completed = (state: JobState, result: unknown, ts: number) => new JobState({ ...state, completedAt: ts, result, status: 'complete' });
  static readonly failed = (state: JobState, error: string, ts: number) => new JobState({ ...state, attempts: state.attempts + 1, completedAt: ts, lastError: error, status: 'failed' });
  static readonly cancelled = (state: JobState, ts: number) => new JobState({ ...state, completedAt: ts, status: 'cancelled' });
}
```
  </action>
  <verify>pnpm exec nx run @parametric-portal/server:typecheck 2>&1 | head -50</verify>
  <done>Imports include Clock, Layer, Stream, Queue, Fiber, Telemetry; schemas defined</done>
</task>

<task type="auto">
  <name>Task 2: Constants, context tag, and entity definition</name>
  <files>packages/server/src/infra/jobs.ts</files>
  <action>
Add after state section.

**Constants Section:**
```typescript
// --- [CONSTANTS] -------------------------------------------------------------

const _CONFIG = {
  entity: { concurrency: 1, mailboxCapacity: 100, maxIdleTime: Duration.minutes(5) },
  pools: { critical: 4, high: 3, normal: 2, low: 1 } as const,
  retry: Schedule.exponential(Duration.millis(100)).pipe(
    Schedule.jittered,
    Schedule.intersect(Schedule.recurs(5)),
    Schedule.upTo(Duration.seconds(30)),
    Schedule.whileInput((e: JobError) => !JobError.isTerminal(e)),
    Schedule.resetAfter(Duration.minutes(5)),
    Schedule.collectAllInputs,
  ),
} as const;
```

**Context Tag (for handler context injection):**
```typescript
// --- [CONTEXT] ---------------------------------------------------------------

class JobContext extends Effect.Tag('JobContext')<JobContext, {
  readonly jobId: string;
  readonly priority: typeof JobPriority.Type;
  readonly reportProgress: (pct: number, message: string) => Effect.Effect<void>;
}>() {}
```

**Entity Definition:**
```typescript
// --- [ENTITY] ----------------------------------------------------------------

const JobEntity = Entity.make('Job', [
  Rpc.make('submit', {
    error: JobError,
    payload: JobPayload.fields,
    primaryKey: (p) => p.dedupeKey ?? null,
    success: S.Struct({ jobId: S.String, duplicate: S.Boolean }),
  }),
  Rpc.make('status', { payload: S.Struct({ jobId: S.String }), success: JobStatusResponse }),
  Rpc.make('progress', {
    payload: S.Struct({ jobId: S.String }),
    success: S.Struct({ pct: S.Number, message: S.String }),
    stream: true,
  }),
  Rpc.make('cancel', { error: JobError, payload: S.Struct({ jobId: S.String }), success: S.Void }),
]);
```
  </action>
  <verify>pnpm exec nx run @parametric-portal/server:typecheck 2>&1 | head -50</verify>
  <done>_CONFIG with pools/retry/collectAllInputs; JobContext with reportProgress; JobEntity with 4 RPCs</done>
</task>

<task type="auto">
  <name>Task 3: Entity layer with complete handler implementations</name>
  <files>packages/server/src/infra/jobs.ts</files>
  <action>
Add JobEntityLive after entity definition. **CRITICAL**: All gaps addressed here.

```typescript
// --- [LAYERS] ----------------------------------------------------------------

const JobEntityLive = JobEntity.toLayer(Effect.gen(function* () {
  const currentAddress = yield* Entity.CurrentAddress;
  const handlers = yield* Ref.make(HashMap.empty<string, (payload: unknown) => Effect.Effect<unknown, unknown, never>>());
  const runningJobs = yield* FiberMap.make<string>();
  const jobStates = yield* Ref.make(HashMap.empty<string, typeof JobStatusResponse.Type>());
  const progressQueue = yield* Queue.sliding<{ jobId: string; pct: number; message: string }>(100);
  const db = yield* DatabaseService;
  const metrics = yield* MetricsService;
  const sharding = yield* Sharding.Sharding;

  // processJob: Wrapped in Telemetry.span, uses Context.Request.tenantId for DLQ
  const processJob = (jobId: string, envelope: typeof JobPayload.Type) =>
    Telemetry.span(
      Context.Request.withinCluster({
        entityId: currentAddress.entityId,
        entityType: currentAddress.entityType,
        shardId: currentAddress.shardId,
      })(Effect.gen(function* () {
        const handler = yield* Ref.get(handlers).pipe(
          Effect.map(HashMap.get(envelope.type)),
          Effect.flatMap(Option.match({
            onNone: () => Effect.fail(JobError.fromHandlerMissing(jobId, envelope.type)),
            onSome: Effect.succeed,
          })),
        );

        const ts = yield* Clock.currentTimeMillis;
        yield* Ref.update(jobStates, HashMap.set(jobId, new JobStatusResponse({
          attempts: 1, history: [{ status: 'processing', timestamp: ts }], status: 'processing',
        })));

        const longJob = envelope.duration === 'long';
        yield* Effect.when(Entity.keepAlive(true), () => longJob);

        // Provide JobContext to handler with reportProgress bound to this job
        yield* Effect.provideService(
          handler(envelope.payload).pipe(Effect.mapError((e) => JobError.fromProcessing(jobId, e))),
          JobContext,
          {
            jobId,
            priority: envelope.priority ?? 'normal',
            reportProgress: (pct, message) => Queue.offer(progressQueue, { jobId, pct, message }),
          },
        ).pipe(
          MetricsService.trackJob({ jobType: envelope.type, operation: 'process', priority: envelope.priority }),
          Effect.ensuring(Effect.when(Entity.keepAlive(false), () => longJob)),
        );

        const completeTs = yield* Clock.currentTimeMillis;
        yield* Ref.update(jobStates, HashMap.modify(jobId, (s) => new JobStatusResponse({
          ...s, history: [...s.history, { status: 'complete', timestamp: completeTs }], status: 'complete',
        })));
        yield* Metric.increment(metrics.jobs.completions);
      }).pipe(
        Effect.catchTag('JobError', (e) => JobError.isTerminal(e)
          ? Effect.gen(function* () {
              // Use Context.Request.tenantId for appId (NOT hardcoded)
              const tenantId = yield* Context.Request.tenantId;
              yield* db.jobDlq.insert({
                appId: tenantId,
                attempts: 1,
                errorHistory: [{ error: String(e.cause), timestamp: Date.now() }],
                errorReason: e.reason,
                originalJobId: jobId,
                payload: envelope.payload,
                type: envelope.type,
              });
              yield* Metric.increment(metrics.jobs.deadLettered);
              return yield* Effect.fail(e);
            })
          : Effect.fail(e)),
      )),
      'job.process',
      { 'job.id': jobId, 'job.type': envelope.type, metrics: false },
    );

  return {
    submit: (envelope) => Effect.gen(function* () {
      const jobId = yield* sharding.getSnowflake.pipe(Effect.map(String));
      yield* Metric.increment(metrics.jobs.enqueued);
      yield* FiberMap.run(runningJobs, jobId)(
        processJob(jobId, envelope.payload).pipe(
          Effect.onInterrupt(() => Effect.logInfo('Job interrupted', { jobId })),
        ),
      );
      return { jobId, duplicate: false };
    }),

    status: (envelope) => Ref.get(jobStates).pipe(
      Effect.map(HashMap.get(envelope.payload.jobId)),
      Effect.flatMap(Option.match({
        onNone: () => Effect.succeed(new JobStatusResponse({ attempts: 0, history: [], status: 'queued' })),
        onSome: Effect.succeed,
      })),
    ),

    progress: (envelope) => Stream.fromQueue(progressQueue).pipe(
      Stream.filter((p) => p.jobId === envelope.payload.jobId),
      Stream.map(({ pct, message }) => ({ pct, message })),
    ),

    // Cancel: Fiber.interrupt + cancellations metric
    cancel: (envelope) => Effect.gen(function* () {
      const fiberOpt = yield* FiberMap.get(runningJobs, envelope.payload.jobId);
      yield* Option.match(fiberOpt, {
        onNone: () => Effect.fail(JobError.fromNotFound(envelope.payload.jobId)),
        onSome: (fiber) => Effect.gen(function* () {
          yield* Fiber.interrupt(fiber);
          yield* FiberMap.remove(runningJobs, envelope.payload.jobId);
          const ts = yield* Clock.currentTimeMillis;
          yield* Ref.update(jobStates, HashMap.modify(envelope.payload.jobId, (s) =>
            new JobStatusResponse({ ...s, history: [...s.history, { status: 'cancelled', timestamp: ts }], status: 'cancelled' }),
          ));
          yield* Metric.increment(metrics.jobs.cancellations);
        }),
      });
    }),
  };
}), {
  concurrency: _CONFIG.entity.concurrency,
  defectRetryPolicy: _CONFIG.retry,
  mailboxCapacity: _CONFIG.entity.mailboxCapacity,
  maxIdleTime: _CONFIG.entity.maxIdleTime,
  spanAttributes: { 'entity.service': 'job-processing', 'entity.version': 'v1' },
});
```
  </action>
  <verify>pnpm exec nx run @parametric-portal/server:typecheck 2>&1 | head -50</verify>
  <done>JobEntityLive: Telemetry.span wraps processJob; Context.Request.tenantId for DLQ; Fiber.interrupt in cancel; cancellations metric; JobContext with reportProgress</done>
</task>

<task type="auto">
  <name>Task 4: JobService facade with const + namespace merge pattern</name>
  <files>packages/server/src/infra/jobs.ts</files>
  <action>
Add JobService as Effect.Service. **CRITICAL**: Use const + namespace merge pattern (NOT static properties on class).

**Service Section (static properties for values, matching cluster.ts:241-246):**
```typescript
// --- [SERVICES] --------------------------------------------------------------

class JobService extends Effect.Service<JobService>()('server/Jobs', {
  dependencies: [JobEntityLive.pipe(Layer.provide(ClusterService.Layer)), DatabaseService.Default, MetricsService.Default],
  scoped: Effect.gen(function* () {
    const sharding = yield* Sharding.Sharding;
    const getClient = yield* sharding.makeClient(JobEntity);
    const handlers = yield* Ref.make(HashMap.empty<string, JobService.Handler>());
    const metrics = yield* MetricsService;
    const db = yield* DatabaseService;
    const counter = yield* Ref.make(0);

    const routeByPriority = (p: keyof typeof _CONFIG.pools) => Effect.gen(function* () {
      const n = yield* Ref.getAndUpdate(counter, (c) => c + 1);
      return `job-${p}-${n % _CONFIG.pools[p]}`;
    });

    const submit = <T>(type: string, payloads: T | readonly T[], opts?: {
      dedupeKey?: string;
      maxAttempts?: number;
      priority?: typeof JobPriority.Type;
    }) => Effect.gen(function* () {
      const items = Array.isArray(payloads) ? payloads : [payloads];
      const priority = opts?.priority ?? 'normal';
      const batchId = items.length > 1 ? crypto.randomUUID() : undefined;
      const results = yield* Effect.forEach(items, (payload, idx) => Effect.gen(function* () {
        const entityId = yield* routeByPriority(priority);
        return yield* Context.Request.withinCluster({ entityId, entityType: 'Job' })(
          getClient(entityId).submit({
            batchId,
            dedupeKey: opts?.dedupeKey ? `${opts.dedupeKey}:${idx}` : undefined,
            maxAttempts: opts?.maxAttempts,
            payload,
            priority,
            type,
          }).pipe(Effect.map((r) => r.jobId)),
        );
      }), { concurrency: 'unbounded' });
      return Array.isArray(payloads) ? results : results[0];
    });

    const enqueue = submit;

    const validateBatch = <T>(items: readonly T[], validator: (item: T) => Effect.Effect<void, JobError>) =>
      Effect.all(
        items.map((item, idx) => validator(item).pipe(Effect.mapError((e) => ({ idx, error: e })))),
        { mode: 'validate', concurrency: 'unbounded' },
      );

    const replay = (dlqId: string) => Effect.gen(function* () {
      const entry = yield* db.jobDlq.get(dlqId);
      yield* Option.match(entry, {
        onNone: () => Effect.fail(JobError.fromNotFound(dlqId)),
        onSome: (e) => submit(e.type, e.payload, { priority: 'normal' }).pipe(
          Effect.zipRight(db.jobDlq.markReplayed(dlqId)),
        ),
      });
    });

    return {
      cancel: (jobId: string) => getClient(jobId).cancel({ jobId }),
      enqueue,
      registerHandler: <T>(type: string, handler: (payload: T) => Effect.Effect<void, unknown, never>) =>
        Ref.update(handlers, HashMap.set(type, handler as JobService.Handler)),
      replay,
      status: (jobId: string) => getClient(jobId).status({ jobId }),
      submit,
      validateBatch,
    };
  }),
}) {
  // Static properties for value access (matching cluster.ts:241-246)
  static readonly Config = _CONFIG;
  static readonly Context = JobContext;
  static readonly Error = JobError;
  static readonly Payload = JobPayload;
  static readonly Response = { Status: JobStatusResponse } as const;
}
```

**Namespace Section (TYPES ONLY - matching cluster.ts:371-394):**
```typescript
// --- [NAMESPACE] -------------------------------------------------------------

namespace JobService {
  export type Handler = (payload: unknown) => Effect.Effect<void, unknown, never>;
  export type Priority = typeof JobPriority.Type;
  export type Status = typeof JobStatus.Type;
  export type Error = InstanceType<typeof JobError>;
  export type Context = Effect.Effect.Context<typeof JobContext>;
}
```

**Export Section (SINGLE EXPORT):**
```typescript
// --- [EXPORT] ----------------------------------------------------------------

export { JobService };
```

Consumers access internal types via `JobService.Error`, `JobService.Context`, etc.
  </action>
  <verify>pnpm exec nx run @parametric-portal/server:typecheck</verify>
  <done>JobService with namespace merge for Context/Error/Payload/Status; single export; validateBatch with mode:'validate'; Context.Request.withinCluster wrapping</done>
</task>

</tasks>

<verification>
1. `pnpm exec nx run @parametric-portal/server:typecheck` passes
2. Single export: `grep '^export' packages/server/src/infra/jobs.ts` shows only `export { JobService };`
3. No poll/SELECT FOR UPDATE: `grep -E 'FOR UPDATE|poll' packages/server/src/infra/jobs.ts` returns empty
4. Entity.make: `grep 'Entity.make' packages/server/src/infra/jobs.ts` returns line
5. File under 275 LOC: `wc -l packages/server/src/infra/jobs.ts` < 275
6. Telemetry.span: `grep 'Telemetry.span' packages/server/src/infra/jobs.ts` returns line
7. Context.Request.tenantId for DLQ: `grep -B2 'db.jobDlq.insert' packages/server/src/infra/jobs.ts` shows tenantId
8. Fiber.interrupt: `grep 'Fiber.interrupt' packages/server/src/infra/jobs.ts` returns line
9. cancellations metric: `grep 'jobs.cancellations' packages/server/src/infra/jobs.ts` returns line
10. reportProgress: `grep 'reportProgress' packages/server/src/infra/jobs.ts` returns lines
11. Schedule.collectAllInputs: `grep 'collectAllInputs' packages/server/src/infra/jobs.ts` returns line
12. validateBatch mode: `grep "mode: 'validate'" packages/server/src/infra/jobs.ts` returns line
13. Namespace pattern: `grep 'namespace JobService' packages/server/src/infra/jobs.ts` returns line
14. Static properties: `grep 'static readonly Context' packages/server/src/infra/jobs.ts` returns line
15. No export const in namespace: `grep -A10 'namespace JobService' packages/server/src/infra/jobs.ts | grep 'export const'` returns empty
</verification>

<success_criteria>
- Single unified export: `export { JobService };` only
- Values exposed via static properties on class (`static readonly Context = JobContext;`)
- Types exposed via namespace (`namespace JobService { export type Error = ... }`)
- Telemetry.span wraps processJob for tracing
- Context.Request.tenantId used for DLQ appId (not hardcoded)
- Fiber.interrupt called in cancel handler
- Metric.increment(metrics.jobs.cancellations) in cancel handler
- JobContext.reportProgress connects handlers to progress queue
- Schedule.collectAllInputs accumulates retry errors
- validateBatch uses Effect.all with mode: 'validate'
- Entity.keepAlive toggled for duration === 'long'
- Context.Request.withinCluster wraps all client calls
- No poll loop, no SELECT FOR UPDATE
- File under 275 LOC
</success_criteria>

<output>
After completion, create `.planning/phases/04-job-processing/04-02-SUMMARY.md`
</output>
