---
phase: 04-job-processing
plan: 02
type: execute
wave: 2
depends_on: ["04-01", "04-03"]
files_modified:
  - packages/server/src/infra/jobs.ts
autonomous: true

must_haves:
  truths:
    - "Jobs submitted via JobService.enqueue appear in database within 100ms"
    - "JobService.cancel(jobId) stops in-flight job and returns cancelled status"
    - "JobService.status(jobId) returns current state from database using existing Job.lastError field"
    - "High-priority jobs complete faster than low-priority jobs under load"
    - "Jobs failing maxAttempts times appear in job_dlq table"
  artifacts:
    - path: "packages/server/src/infra/jobs.ts"
      provides: "Entity-based JobService with polymorphic enqueue"
      exports: ["JobService"]
      contains: "Entity.make"
      min_lines: 100
      max_lines: 250
  key_links:
    - from: "packages/server/src/infra/jobs.ts"
      to: "packages/server/src/infra/cluster.ts"
      via: "ClusterService import"
      pattern: "import.*ClusterService.*from.*cluster"
    - from: "packages/server/src/infra/jobs.ts"
      to: "packages/database/src/repos.ts"
      via: "DatabaseService for job persistence and DLQ"
      pattern: "import.*DatabaseService.*from.*database"
    - from: "packages/server/src/infra/jobs.ts"
      to: "packages/server/src/observe/metrics.ts"
      via: "MetricsService for job.* metrics"
      pattern: "import.*MetricsService.*from.*metrics"
---

<objective>
Gut and replace jobs.ts with Entity-based job dispatch.

Purpose: Replace poll-based job queue with instant Entity mailbox dispatch. Remove SELECT FOR UPDATE patterns. Preserve JobService interface for existing callers.
Output: Complete JobService rewrite under 250 LOC using Entity pattern from Phase 1
</objective>

<execution_context>
@/Users/bardiasamiee/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bardiasamiee/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-job-processing/04-RESEARCH.md
@.planning/phases/04-job-processing/04-CONTEXT.md
@.planning/phases/04-job-processing/04-01-SUMMARY.md
@.planning/phases/04-job-processing/04-03-SUMMARY.md
@packages/server/src/infra/cluster.ts
@packages/server/src/infra/jobs.ts
@packages/server/src/utils/resilience.ts
@packages/server/src/context.ts
@packages/database/src/repos.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Define JobEntity schemas, errors, and Entity.make</name>
  <files>packages/server/src/infra/jobs.ts</files>
  <action>
Replace the entire file with new Entity-based implementation. Start with schemas and entity definition.

**Section: SCHEMA**
Define typed schemas following Phase 1 ClusterEntity pattern:
- JobPayload: type, payload, priority (optional Literal), batchId (optional)
- JobStatus: Literal('queued', 'processing', 'complete', 'failed', 'cancelled')
- JobStatusResponse: status, attempts, lastError (optional)

**Section: ERRORS**
Define JobError as S.TaggedError with reasons:
- NotFound, AlreadyCancelled, HandlerMissing, Validation, Processing
Use static factory methods following ClusterError pattern.

**Section: ENTITY**
Create JobEntity using Entity.make("Job", [...]):
- enqueue: Rpc.make (preserves existing interface)
- status: Rpc.make returning JobStatusResponse
- cancel: Rpc.make with JobError

**Deduplication note:** Deduplication is handled at the RPC layer via `Rpc.make({ primaryKey })` which uses @effect/cluster's SqlMessageStorage internally. This is MESSAGE-level deduplication, not a separate database field. When the same primaryKey is submitted twice, the cluster returns the existing result rather than creating a duplicate job.

**Section: CONSTANTS**
Define _CONFIG matching cluster.ts style:
- Priority pool sizing: { critical: 4, high: 3, normal: 2, low: 1 }
- Retry: use Resilience.schedules.default
- maxIdleTime for keepAlive threshold
  </action>
  <verify>`pnpm exec nx run server:typecheck` passes</verify>
  <done>JobEntity defined with typed RPC messages, JobError with factories</done>
</task>

<task type="auto">
  <name>Task 2: Implement JobEntityLive layer with handler and database persistence</name>
  <files>packages/server/src/infra/jobs.ts</files>
  <action>
Add JobEntityLive layer using Entity.toLayer following cluster.ts pattern.

**Handler implementations:**
1. enqueue handler:
   - Wrap in Context.Request.withinCluster (from Phase 2 pattern)
   - Persist job record to database via db.jobs.put({ id, type, payload, status: 'queued', attempts: 0 })
   - Get handler from registered handlers Ref
   - Update job status in database: db.jobs.set(id, { status: 'processing' })
   - Execute handler with proper error handling
   - On success: db.jobs.complete(id) (uses existing repo method)
   - On failure: check attempts vs maxAttempts
   - If not exhausted: db.jobs.retry(id, { attempts, lastError, scheduledAt })
   - If exhausted: db.jobDlq.put(...) and db.jobs.deadLetter(id, error)
   - Use existing Job.lastError field for error tracking (no history array needed — lastError stores most recent error)

2. status handler:
   - Fetch from database: db.jobs.get(id)
   - Return JobStatusResponse with status, attempts, lastError from persisted record
   - Return JobError.NotFound if not in database

3. cancel handler:
   - Get fiber from runningJobs Ref (in-memory for active fibers only)
   - Call Fiber.interrupt on running fiber
   - Update database: db.jobs.set(id, { status: 'cancelled' })

**Job results note:** Job handlers execute side effects and return void. Results are not persisted — this is fire-and-forget with side effects (like sending emails, updating external systems). The handler's return value is discarded after completion.

**Entity options:**
- concurrency: 1 (per entity)
- mailboxCapacity: 100
- maxIdleTime: Duration.minutes(5)
- defectRetryPolicy: Schedule.exponential with jitter (from resilience.ts)
- spanAttributes for telemetry

**Fiber tracking (in-memory only):**
- Use Ref.make(HashMap.empty) for running fibers (for cancellation)
- Fork job execution, store fiber reference
- Remove fiber from Ref on completion/failure
  </action>
  <verify>`pnpm exec nx run server:typecheck` passes AND `pnpm exec nx run database:typecheck` passes</verify>
  <done>JobEntityLive layer persists job lifecycle to database, enables status retrieval</done>
</task>

<task type="auto">
  <name>Task 3: Implement JobService facade with priority routing and database operations</name>
  <files>packages/server/src/infra/jobs.ts</files>
  <action>
Create JobService class using Effect.Service pattern following cluster.ts.

**Service methods:**
1. enqueue<T>(type, payloads: T | readonly T[], opts?):
   - IMPORTANT: Keep method name as `enqueue` to preserve existing interface compatibility
   - Polymorphic: detect array vs single
   - Route to priority-specific entity via routeByPriority
   - Generate batchId for batches
   - Fire-and-forget: return job ID(s) immediately
   - Track metrics: MetricsService.jobs.enqueued with { type, priority } labels

2. status(jobId):
   - Route to entity, call status RPC
   - Return JobStatusResponse (status, attempts, lastError from database)

3. cancel(jobId):
   - Route to entity, call cancel RPC
   - Track metrics: MetricsService.jobs.cancellations
   - Return void on success, JobError on failure

4. registerHandler(type, handler):
   - Store in handlers Ref
   - Used by JobEntityLive to dispatch

**Priority routing semantics:**
Priority affects processing CAPACITY, not queue ordering. Higher priority jobs get more Entity instances, meaning more concurrent processing capacity:
```typescript
// Pool sizes per priority level
const _pools = { critical: 4, high: 3, normal: 2, low: 1 } as const;

// Route job to random entity within its priority pool
const routeByPriority = (priority: keyof typeof _pools) =>
  `job-${priority}-${Math.floor(Math.random() * _pools[priority])}`;
```
- critical: 4 entities = 4x concurrent capacity
- high: 3 entities = 3x concurrent capacity
- normal: 2 entities = 2x concurrent capacity
- low: 1 entity = baseline capacity

Under load, critical jobs complete faster because they have more processing entities available.

**Dependencies:**
- ClusterService.Layer (provides sharding)
- DatabaseService (for job persistence via db.jobs and DLQ via db.jobDlq)
- MetricsService (for job.* metrics from 04-03)

**Database operations used (all existing methods):**
- db.jobs.insert(job) - Create new job record
- db.jobs.get(id) - Retrieve job with lastError
- db.jobs.set(id, updates) - Update status/attempts
- db.jobs.complete(id) - Mark completed
- db.jobs.retry(id, opts) - Retry with lastError
- db.jobs.deadLetter(id, error) - Move to dead status
- db.jobDlq.insert(dlqRecord) - Dead-letter failed job

**Namespace exports:**
- JobService.Handler type
- Remove StatusEvent (no longer using pg.listen)
  </action>
  <verify>`pnpm exec nx run server:typecheck` passes</verify>
  <done>JobService facade provides unchanged interface with Entity-based dispatch and database persistence</done>
</task>

</tasks>

<verification>
1. `pnpm exec nx run server:typecheck` - Type checking passes
2. `pnpm exec nx run database:typecheck` - Database types validate
3. File under 250 LOC: `wc -l packages/server/src/infra/jobs.ts`
4. No `SELECT FOR UPDATE` or poll patterns: `grep -c "SELECT FOR UPDATE\|Schedule.spaced\|Schedule.repeat" packages/server/src/infra/jobs.ts` returns 0
5. Database persistence present: `grep -c "db.jobs" packages/server/src/infra/jobs.ts` returns 3+ matches
6. JobService.enqueue method exists (interface unchanged)
</verification>

<success_criteria>
- Existing callers using JobService.enqueue still compile (method name preserved)
- Entity mailbox dispatch replaces poll-based queue
- Priority routing distributes jobs across weighted entity pools (more capacity = faster completion)
- Deduplication via Rpc.make primaryKey (message-level, handled by @effect/cluster)
- Job lifecycle persisted to database using existing Job model fields (status, attempts, lastError)
- DLQ integration for failed jobs via DatabaseService.jobDlq
- Cancellation via fiber interruption with database status update
- File under 250 LOC
</success_criteria>

<output>
After completion, create `.planning/phases/04-job-processing/04-02-SUMMARY.md`
</output>
